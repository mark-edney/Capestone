download.file(url3,destfile = "~/R/Capestone/data/diction.txt")
setwd("~/R/Capestone/data/")
unzip("~/R/Capestone/data/prof.zip")
unzip("~/R/Capestone/data/data.zip")
setwd("~/R/Capestone")
}
blog <- read_lines("~/R/Capestone/data/final/en_US/en_US.blogs.txt")
news <- read_lines("~/R/Capestone/data/final/en_US/en_US.news.txt")
twitter <- read_lines("~/R/Capestone/data/final/en_US/en_US.twitter.txt")
blog <- tibble(text = blog)
news <- tibble(text = news)
twitter <- tibble(text = twitter)
set.seed(90210)
corpus <- full_join(blog,twitter) %>% full_join(news) %>%
slice_sample(prop = 0.10)
prof <- read_lines("~/R/Capestone/data/facebook-bad-words-list_comma-separated-text-file_2021_01_18.txt")[15]
prof <- prof %>% str_split(", ") %>% flatten %>% unlist
prof <- tibble("word" = prof)
english <- read_lines("~/R/Capestone/data/diction.txt")
english <- tibble("word" = english[!english==""])
#clean up ram
rm(blog,news,twitter)
voc <- english %>% anti_join(prof) %>% anti_join(stop_words)
unigram <- corpus %>% unnest_tokens(ngram, text, token = "ngrams", n = 1) %>%
semi_join(voc, by = c("ngram"="word"))
unigramcount <- count(unigram, ngram)
#decreases the voc size
voc <- tibble(word = unigramcount$ngram)
Nr <- count(temp, n) %>% add_row(n = 0, nn = 0)  %>%
arrange(n)
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = TRUE)
library(tidyverse)
library(tidytext)
library(stopwords)
#downloads the corpus files, profanity filter and English dictionary
url <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
url2 <- "https://www.freewebheaders.com/download/files/facebook-bad-words-list_comma-separated-text-file_2021_01_18.zip"
url3 <- "https://raw.githubusercontent.com/dwyl/english-words/master/words_alpha.txt"
if(dir.exists("~/R/Capestone/data/") == FALSE){
dir.create("~/R/Capestone/data/")}
if(file.exists("~/R/Capestone/data/data.zip") == FALSE|
file.exists("~/R/Capestone/data/prof.zip")==FALSE|
file.exists("~/R/Capestone/data/diction.txt")==FALSE){
download.file(url,destfile = "~/R/Capestone/data/data.zip")
download.file(url2,destfile = "~/R/Capestone/data/prof.zip")
download.file(url3,destfile = "~/R/Capestone/data/diction.txt")
setwd("~/R/Capestone/data/")
unzip("~/R/Capestone/data/prof.zip")
unzip("~/R/Capestone/data/data.zip")
setwd("~/R/Capestone")
}
blog <- read_lines("~/R/Capestone/data/final/en_US/en_US.blogs.txt")
news <- read_lines("~/R/Capestone/data/final/en_US/en_US.news.txt")
twitter <- read_lines("~/R/Capestone/data/final/en_US/en_US.twitter.txt")
blog <- tibble(text = blog)
news <- tibble(text = news)
twitter <- tibble(text = twitter)
set.seed(90210)
corpus <- full_join(blog,twitter) %>% full_join(news) %>%
slice_sample(prop = 0.10)
prof <- read_lines("~/R/Capestone/data/facebook-bad-words-list_comma-separated-text-file_2021_01_18.txt")[15]
prof <- prof %>% str_split(", ") %>% flatten %>% unlist
prof <- tibble("word" = prof)
english <- read_lines("~/R/Capestone/data/diction.txt")
english <- tibble("word" = english[!english==""])
#clean up ram
rm(blog,news,twitter)
voc <- english %>% anti_join(prof) %>% anti_join(stop_words)
unigram <- corpus %>% unnest_tokens(ngram, text, token = "ngrams", n = 1) %>%
semi_join(voc, by = c("ngram"="word"))
unigramcount <- count(unigram, ngram)
#decreases the voc size
voc <- tibble(word = unigramcount$ngram)
#OOV 1% of the least likely unigrams
unks <- unigramcount[unigramcount$n==1,] %>% slice_sample(prop = 0.01)
unigram[unigram$ngram %in% unks$ngram,]$ngram <- "<unk>"
unigramcount <- count(unigram, ngram)
remove <- unigramcount[unigramcount$n==1,]
unigram <- unigram[!unigram$ngram %in% remove$ngram,]
unigram <- count(unigram, ngram)
voc <- tibble(word = unigram$ngram)
bigram <- corpus %>% unnest_tokens(ngram, text, token = "ngrams", n = 2) %>%
separate(ngram, c("word1", "word2"))
bigram$word1[bigram$word1 %in% unks$ngram] <- "<unk>"
bigram$word2[bigram$word2 %in% unks$ngram] <- "<unk>"
bigram <- bigram %>% filter(word1 %in% voc$word, word2 %in% voc$word) %>%
count(word1, word2)
View(remove)
View(remove)
trigram <- corpus %>% unnest_tokens(ngram, text, token = "ngrams", n = 3) %>%
separate(ngram, c("word1", "word2", "word3"))
trigram$word1[trigram$word1 %in% unks$ngram] <- "<unk>"
trigram$word2[trigram$word2 %in% unks$ngram] <- "<unk>"
trigram$word3[trigram$word3 %in% unks$ngram] <- "<unk>"
trigram <- trigram %>%
filter(word1 %in% voc$word, word2 %in% voc$word, word3 %in% voc$word) %>%
count(word1, word2, word3)
dist <- tibble(word = voc$word)
input <- tibble(text = c("When you breathe, I want to be the air for you. I'll be there for you, I'd live and I'd",
"Guy at my table's wife got up to go to the bathroom and I asked about dessert and he started telling me about his",
"I'd give anything to see arctic monkeys this",
"Talking to your mom has the same effect as a hug and helps reduce your",
"When you were in Holland you were like 1 inch away from me but you hadn't time to take a",
"I'd just like all of these questions answered, a presentation of evidence, and a jury to settle the",
"I can't deal with unsymetrical things. I can't even hold an uneven number of bags of groceries in each",
"Every inch of you is perfect from the bottom to the",
"Iâ€™m thankful my childhood was filled with imagination and bruises from playing",
"I like how the same people are in almost all of Adam Sandler's")) %>%
mutate(line = row_number())
input <- unnest_tokens(input, word, text)
input <- split(input, input$line)
w <- vector(length = 3)
w[1] <- input$'1'[nrow(input$'1')-1,]$word
w[2] <- input$'1'[nrow(input$'1'),]$word
temp <- trigram %>% filter(word1 == w[1]) %>% filter(word2 == w[2])
MLEdf <- tibble(word=temp$word3, MLE=temp$n/sum(temp$n))
dist <- left_join(dist, MLEdf)
dist[is.na(dist$MLE),]$MLE <- 0
ADDdf <- tibble(word=temp$word3, ADD=temp$n)
dist <- left_join(dist, ADDdf)
dist[is.na(dist$ADD),]$ADD <- 0
dist$ADD <- dist$ADD + 1
dist$ADD <- dist$ADD / sum(dist$ADD)
stop_words
voc <- english
voc <- english
voc <- english %>% anti_join(prof)
voc <- english %>% anti_join(prof) %>% anti_join(stop_words)
voc <- english %>% anti_join(prof)
View(MLEdf)
View(dist)
summarise(dist, sum())
summarise_all(dist, sum())
summarise_all(dist
)
summarise(dist)
summarise(dist, sum())
summarise(dist, sum)
sum(dist$MLE)
sum(dist$ADD)
u
w
10 %x% 5
10 %x% 0.2
?%x%
set.seed(1987, sample.kind="Rounding")
n <- 100
k <- 8
Sigma <- 64  * matrix(c(1, .75, .5, .75, 1, .5, .5, .5, 1), 3, 3)
m <- MASS::mvrnorm(n, rep(0, 3), Sigma)
m
m <- m[order(rowMeans(m), decreasing = TRUE),]
m
y <- m %x% matrix(rep(1, k), nrow = 1) + matrix(rnorm(matrix(n*k*3)), n, k*3)
y
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = TRUE)
library(tidyverse)
library(tidytext)
library(stopwords)
#downloads the corpus files, profanity filter and English dictionary
url <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
url2 <- "https://www.freewebheaders.com/download/files/facebook-bad-words-list_comma-separated-text-file_2021_01_18.zip"
url3 <- "https://raw.githubusercontent.com/dwyl/english-words/master/words_alpha.txt"
if(dir.exists("~/R/Capestone/data/") == FALSE){
dir.create("~/R/Capestone/data/")}
if(file.exists("~/R/Capestone/data/data.zip") == FALSE|
file.exists("~/R/Capestone/data/prof.zip")==FALSE|
file.exists("~/R/Capestone/data/diction.txt")==FALSE){
download.file(url,destfile = "~/R/Capestone/data/data.zip")
download.file(url2,destfile = "~/R/Capestone/data/prof.zip")
download.file(url3,destfile = "~/R/Capestone/data/diction.txt")
setwd("~/R/Capestone/data/")
unzip("~/R/Capestone/data/prof.zip")
unzip("~/R/Capestone/data/data.zip")
setwd("~/R/Capestone")
}
blog <- read_lines("~/R/Capestone/data/final/en_US/en_US.blogs.txt")
unzip("~/R/Capestone/data/data.zip")
unzip("~/R/Capestone/data/data.zip")
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = TRUE)
library(tidyverse)
library(tidytext)
library(stopwords)
#downloads the corpus files, profanity filter and English dictionary
url <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
url2 <- "https://www.freewebheaders.com/download/files/facebook-bad-words-list_comma-separated-text-file_2021_01_18.zip"
url3 <- "https://raw.githubusercontent.com/dwyl/english-words/master/words_alpha.txt"
if(dir.exists("~/R/Capestone/data/") == FALSE){
dir.create("~/R/Capestone/data/")}
if(file.exists("~/R/Capestone/data/data.zip") == FALSE|
file.exists("~/R/Capestone/data/prof.zip")==FALSE|
file.exists("~/R/Capestone/data/diction.txt")==FALSE){
download.file(url,destfile = "~/R/Capestone/data/data.zip")
download.file(url2,destfile = "~/R/Capestone/data/prof.zip")
download.file(url3,destfile = "~/R/Capestone/data/diction.txt")
setwd("~/R/Capestone/data/")
unzip("~/R/Capestone/data/prof.zip")
unzip("~/R/Capestone/data/data.zip")
setwd("~/R/Capestone")
}
blog <- read_lines("~/R/Capestone/data/final/en_US/en_US.blogs.txt")
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = TRUE)
library(tidyverse)
library(tidytext)
library(stopwords)
#downloads the corpus files, profanity filter and English dictionary
url <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
url2 <- "https://www.freewebheaders.com/download/files/facebook-bad-words-list_comma-separated-text-file_2021_01_18.zip"
url3 <- "https://raw.githubusercontent.com/dwyl/english-words/master/words_alpha.txt"
if(dir.exists("~/R/Capestone/data/") == FALSE){
dir.create("~/R/Capestone/data/")}
if(file.exists("~/R/Capestone/data/data.zip") == FALSE|
file.exists("~/R/Capestone/data/prof.zip")==FALSE|
file.exists("~/R/Capestone/data/diction.txt")==FALSE){
download.file(url,destfile = "~/R/Capestone/data/data.zip")
download.file(url2,destfile = "~/R/Capestone/data/prof.zip")
download.file(url3,destfile = "~/R/Capestone/data/diction.txt")
setwd("~/R/Capestone/data/")
unzip("~/R/Capestone/data/prof.zip")
unzip("~/R/Capestone/data/data.zip")
setwd("~/R/Capestone")
}
blog <- read_lines("~/R/Capestone/data/final/en_US/en_US.blogs.txt")
news <- read_lines("~/R/Capestone/data/final/en_US/en_US.news.txt")
twitter <- read_lines("~/R/Capestone/data/final/en_US/en_US.twitter.txt")
blog <- tibble(text = blog)
news <- tibble(text = news)
twitter <- tibble(text = twitter)
set.seed(90210)
corpus <- full_join(blog,twitter) %>% full_join(news) %>%
slice_sample(prop = 0.10)
prof <- read_lines("~/R/Capestone/data/facebook-bad-words-list_comma-separated-text-file_2021_01_18.txt")[15]
prof <- prof %>% str_split(", ") %>% flatten %>% unlist
prof <- tibble("word" = prof)
english <- read_lines("~/R/Capestone/data/diction.txt")
english <- tibble("word" = english[!english==""])
#clean up ram
rm(blog,news,twitter)
voc <- english %>% anti_join(prof) %>% anti_join(stop_words)
unigram <- corpus %>% unnest_tokens(ngram, text, token = "ngrams", n = 1) %>%
semi_join(voc, by = c("ngram"="word"))
#decreases the voc size
voc <- tibble(word = unigram$ngram)
#OOV 1% of the least likely unigrams
unk <- unigram %>%
count(ngram) %>%
filter(n==1) %>%
slice_sample(prop = 0.005)
unigram[unigram$ngram %in% unk$ngram,]$ngram <- "<unk>"
unigram <- count(unigram, ngram)
remove <- unigram$n==1
unigram <- unigram[!remove,]
voc <- tibble(word = unigram$ngram)
bigram <- corpus %>% unnest_tokens(ngram, text, token = "ngrams", n = 2) %>%
separate(ngram, c("word1", "word2"))
bigram$word1[bigram$word1 %in% unks$ngram] <- "<unk>"
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = TRUE)
library(tidyverse)
library(tidytext)
library(stopwords)
#downloads the corpus files, profanity filter and English dictionary
url <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
url2 <- "https://www.freewebheaders.com/download/files/facebook-bad-words-list_comma-separated-text-file_2021_01_18.zip"
url3 <- "https://raw.githubusercontent.com/dwyl/english-words/master/words_alpha.txt"
if(dir.exists("~/R/Capestone/data/") == FALSE){
dir.create("~/R/Capestone/data/")}
if(file.exists("~/R/Capestone/data/data.zip") == FALSE|
file.exists("~/R/Capestone/data/prof.zip")==FALSE|
file.exists("~/R/Capestone/data/diction.txt")==FALSE){
download.file(url,destfile = "~/R/Capestone/data/data.zip")
download.file(url2,destfile = "~/R/Capestone/data/prof.zip")
download.file(url3,destfile = "~/R/Capestone/data/diction.txt")
setwd("~/R/Capestone/data/")
unzip("~/R/Capestone/data/prof.zip")
unzip("~/R/Capestone/data/data.zip")
setwd("~/R/Capestone")
}
blog <- read_lines("~/R/Capestone/data/final/en_US/en_US.blogs.txt")
news <- read_lines("~/R/Capestone/data/final/en_US/en_US.news.txt")
twitter <- read_lines("~/R/Capestone/data/final/en_US/en_US.twitter.txt")
blog <- tibble(text = blog)
news <- tibble(text = news)
twitter <- tibble(text = twitter)
set.seed(90210)
corpus <- full_join(blog,twitter) %>% full_join(news) %>%
slice_sample(prop = 0.10)
prof <- read_lines("~/R/Capestone/data/facebook-bad-words-list_comma-separated-text-file_2021_01_18.txt")[15]
prof <- prof %>% str_split(", ") %>% flatten %>% unlist
prof <- tibble("word" = prof)
english <- read_lines("~/R/Capestone/data/diction.txt")
english <- tibble("word" = english[!english==""])
#clean up ram
rm(blog,news,twitter)
voc <- english %>% anti_join(prof) %>% anti_join(stop_words)
unigram <- corpus %>% unnest_tokens(ngram, text, token = "ngrams", n = 1) %>%
semi_join(voc, by = c("ngram"="word"))
#decreases the voc size
voc <- tibble(word = unigram$ngram)
#OOV 1% of the least likely unigrams
unk <- unigram %>%
count(ngram) %>%
filter(n==1) %>%
slice_sample(prop = 0.005)
unigram[unigram$ngram %in% unk$ngram,]$ngram <- "<unk>"
unigram <- count(unigram, ngram)
remove <- unigram$n==1
unigram <- unigram[!remove,]
voc <- tibble(word = unigram$ngram)
bigram <- corpus %>% unnest_tokens(ngram, text, token = "ngrams", n = 2) %>%
separate(ngram, c("word1", "word2"))
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = TRUE)
library(tidyverse)
library(tidytext)
library(stopwords)
#downloads the corpus files, profanity filter and English dictionary
url <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
url2 <- "https://www.freewebheaders.com/download/files/facebook-bad-words-list_comma-separated-text-file_2021_01_18.zip"
url3 <- "https://raw.githubusercontent.com/dwyl/english-words/master/words_alpha.txt"
if(dir.exists("~/R/Capestone/data/") == FALSE){
dir.create("~/R/Capestone/data/")}
if(file.exists("~/R/Capestone/data/data.zip") == FALSE|
file.exists("~/R/Capestone/data/prof.zip")==FALSE|
file.exists("~/R/Capestone/data/diction.txt")==FALSE){
download.file(url,destfile = "~/R/Capestone/data/data.zip")
download.file(url2,destfile = "~/R/Capestone/data/prof.zip")
download.file(url3,destfile = "~/R/Capestone/data/diction.txt")
setwd("~/R/Capestone/data/")
unzip("~/R/Capestone/data/prof.zip")
unzip("~/R/Capestone/data/data.zip")
setwd("~/R/Capestone")
}
blog <- read_lines("~/R/Capestone/data/final/en_US/en_US.blogs.txt")
news <- read_lines("~/R/Capestone/data/final/en_US/en_US.news.txt")
twitter <- read_lines("~/R/Capestone/data/final/en_US/en_US.twitter.txt")
blog <- tibble(text = blog)
news <- tibble(text = news)
twitter <- tibble(text = twitter)
set.seed(90210)
corpus <- full_join(blog,twitter) %>% full_join(news) %>%
slice_sample(prop = 0.10)
prof <- read_lines("~/R/Capestone/data/facebook-bad-words-list_comma-separated-text-file_2021_01_18.txt")[15]
prof <- prof %>% str_split(", ") %>% flatten %>% unlist
prof <- tibble("word" = prof)
english <- read_lines("~/R/Capestone/data/diction.txt")
english <- tibble("word" = english[!english==""])
#clean up ram
rm(blog,news,twitter)
voc <- english %>% anti_join(prof) %>% anti_join(stop_words)
unigram <- corpus %>% unnest_tokens(ngram, text, token = "ngrams", n = 1) %>%
semi_join(voc, by = c("ngram"="word"))
#decreases the voc size
voc <- tibble(word = unigram$ngram)
#OOV 1% of the least likely unigrams
unk <- unigram %>%
count(ngram) %>%
filter(n==1) %>%
slice_sample(prop = 0.005)
unigram[unigram$ngram %in% unk$ngram,]$ngram <- "<unk>"
unigram <- count(unigram, ngram)
remove <- unigram$n==1
unigram <- unigram[!remove,]
voc <- tibble(word = unigram$ngram)
bigram <- corpus %>% unnest_tokens(ngram, text, token = "ngrams", n = 2) %>%
separate(ngram, c("word1", "word2"))
bigram$word1[bigram$word1 %in% unk$ngram] <- "<unk>"
bigram$word2[bigram$word2 %in% unk$ngram] <- "<unk>"
bigram <- bigram %>% filter(word1 %in% voc$word, word2 %in% voc$word) %>%
count(word1, word2)
trigram <- corpus %>% unnest_tokens(ngram, text, token = "ngrams", n = 3) %>%
separate(ngram, c("word1", "word2", "word3"))
trigram$word1[trigram$word1 %in% unk$ngram] <- "<unk>"
trigram$word2[trigram$word2 %in% unk$ngram] <- "<unk>"
trigram$word3[trigram$word3 %in% unk$ngram] <- "<unk>"
trigram <- trigram %>%
filter(word1 %in% voc$word, word2 %in% voc$word, word3 %in% voc$word) %>%
count(word1, word2, word3)
dist <- tibble(word = voc$word)
input <- tibble(text = c("When you breathe, I want to be the air for you. I'll be there for you, I'd live and I'd",
"Guy at my table's wife got up to go to the bathroom and I asked about dessert and he started telling me about his",
"I'd give anything to see arctic monkeys this",
"Talking to your mom has the same effect as a hug and helps reduce your",
"When you were in Holland you were like 1 inch away from me but you hadn't time to take a",
"I'd just like all of these questions answered, a presentation of evidence, and a jury to settle the",
"I can't deal with unsymetrical things. I can't even hold an uneven number of bags of groceries in each",
"Every inch of you is perfect from the bottom to the",
"Iâ€™m thankful my childhood was filled with imagination and bruises from playing",
"I like how the same people are in almost all of Adam Sandler's")) %>%
mutate(line = row_number())
input <- unnest_tokens(input, word, text)
input <- split(input, input$line)
w <- vector(length = 3)
w[1] <- input$'1'[nrow(input$'1')-1,]$word
w[2] <- input$'1'[nrow(input$'1'),]$word
temp <- trigram %>% filter(word1 == w[1]) %>% filter(word2 == w[2])
MLEdf <- tibble(word=temp$word3, MLE=temp$n/sum(temp$n))
dist <- left_join(dist, MLEdf)
dist[is.na(dist$MLE),]$MLE <- 0
ADDdf <- tibble(word=temp$word3, ADD=temp$n)
dist <- left_join(dist, ADDdf)
dist[is.na(dist$ADD),]$ADD <- 0
dist$ADD <- dist$ADD + 1
dist$ADD <- dist$ADD / sum(dist$ADD)
Nr <- count(temp, n) %>% add_row(n = 0, nn = 0)  %>%
arrange(n)
Nr %<>% mutate(c= 0) %>% mutate(Gt = 0)
total <- sum(Nr$nn*Nr$n)
Nr$Gt[Nr$n==0] <- Nr$nn[Nr$n==1] / total
bigram
trigram
w
trigram[trigram$word1==w[1]]
trigram[trigram$word1==w[1],]
w[1]
setwd("~/R/Capestone")
setwd("~/R/Capestone")
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = TRUE)
library(tidyverse)
library(tidytext)
#downloads the corpus files, profanity filter and English dictionary
url <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
url2 <- "https://www.freewebheaders.com/download/files/facebook-bad-words-list_comma-separated-text-file_2021_01_18.zip"
url3 <- "https://raw.githubusercontent.com/dwyl/english-words/master/words_alpha.txt"
if(dir.exists("~/R/Capestone/data/") == FALSE){
dir.create("~/R/Capestone/data/")}
if(file.exists("~/R/Capestone/data/data.zip") == FALSE|
file.exists("~/R/Capestone/data/prof.zip")==FALSE|
file.exists("~/R/Capestone/data/diction.txt")==FALSE){
download.file(url,destfile = "~/R/Capestone/data/data.zip")
download.file(url2,destfile = "~/R/Capestone/data/prof.zip")
download.file(url3,destfile = "~/R/Capestone/data/diction.txt")
setwd("~/R/Capestone/data/")
unzip("~/R/Capestone/data/prof.zip")
unzip("~/R/Capestone/data/data.zip")
setwd("~/R/Capestone")
}
blog <- read_lines("~/R/Capestone/data/final/en_US/en_US.blogs.txt")
news <- read_lines("~/R/Capestone/data/final/en_US/en_US.news.txt")
twitter <- read_lines("~/R/Capestone/data/final/en_US/en_US.twitter.txt")
blog <- tibble(text = blog)
news <- tibble(text = news)
twitter <- tibble(text = twitter)
set.seed(90210)
corpus <- full_join(blog,twitter) %>% full_join(news) %>%
slice_sample(prop = 0.10)
prof <- read_lines("~/R/Capestone/data/facebook-bad-words-list_comma-separated-text-file_2021_01_18.txt")[15]
prof <- prof %>% str_split(", ") %>% flatten %>% unlist
prof <- tibble("word" = prof)
english <- read_lines("~/R/Capestone/data/diction.txt")
english <- tibble("word" = english[!english==""])
#clean up ram
rm(blog,news,twitter)
voc <- english %>% anti_join(prof)
unigram <- corpus %>% unnest_tokens(ngram, text, token = "ngrams", n = 1) %>%
semi_join(voc, by = c("ngram"="word"))
#decreases the voc size
voc <- tibble(word = unigram$ngram)
#OOV 1% of the least likely unigrams
unk <- unigram %>%
count(ngram) %>%
filter(n==1) %>%
slice_sample(prop = 0.005)
unigram[unigram$ngram %in% unk$ngram,]$ngram <- "<unk>"
unigram <- count(unigram, ngram)
remove <- unigram$n==1
unigram <- unigram[!remove,]
voc <- tibble(word = unigram$ngram)
bigram <- corpus %>% unnest_tokens(ngram, text, token = "ngrams", n = 2) %>%
separate(ngram, c("word1", "word2"))
bigram$word1[bigram$word1 %in% unk$ngram] <- "<unk>"
bigram$word2[bigram$word2 %in% unk$ngram] <- "<unk>"
bigram <- bigram %>% filter(word1 %in% voc$word, word2 %in% voc$word) %>%
count(word1, word2)
trigram <- corpus %>% unnest_tokens(ngram, text, token = "ngrams", n = 3) %>%
separate(ngram, c("word1", "word2", "word3"))
trigram$word1[trigram$word1 %in% unk$ngram] <- "<unk>"
trigram$word2[trigram$word2 %in% unk$ngram] <- "<unk>"
trigram$word3[trigram$word3 %in% unk$ngram] <- "<unk>"
trigram <- trigram %>%
filter(word1 %in% voc$word, word2 %in% voc$word, word3 %in% voc$word) %>%
count(word1, word2, word3)
dist <- tibble(word = voc$word)
input <- tibble(text = c("When you breathe, I want to be the air for you. I'll be there for you, I'd live and I'd",
"Guy at my table's wife got up to go to the bathroom and I asked about dessert and he started telling me about his",
"I'd give anything to see arctic monkeys this",
"Talking to your mom has the same effect as a hug and helps reduce your",
"When you were in Holland you were like 1 inch away from me but you hadn't time to take a",
"I'd just like all of these questions answered, a presentation of evidence, and a jury to settle the",
"I can't deal with unsymetrical things. I can't even hold an uneven number of bags of groceries in each",
"Every inch of you is perfect from the bottom to the",
"Iâ€™m thankful my childhood was filled with imagination and bruises from playing",
"I like how the same people are in almost all of Adam Sandler's")) %>%
mutate(line = row_number())
input <- unnest_tokens(input, word, text)
input <- split(input, input$line)
w <- vector(length = 3)
w[1] <- input$'1'[nrow(input$'1')-1,]$word
w[2] <- input$'1'[nrow(input$'1'),]$word
temp <- trigram %>% filter(word1 == w[1]) %>% filter(word2 == w[2])
MLEdf <- tibble(word=temp$word3, MLE=temp$n/sum(temp$n))
dist <- left_join(dist, MLEdf)
dist[is.na(dist$MLE),]$MLE <- 0
ADDdf <- tibble(word=temp$word3, ADD=temp$n)
dist <- left_join(dist, ADDdf)
dist[is.na(dist$ADD),]$ADD <- 0
dist$ADD <- dist$ADD + 1
dist$ADD <- dist$ADD / sum(dist$ADD)
dist
w[1]
trigram
trigram[trigram$word2==w[2],]
trigram[trigram$word1==w[1],]
w[2]
