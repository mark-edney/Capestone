setwd("~/R/Capestone/data/")
unzip("~/R/Capestone/data/prof.zip")
unzip("~/R/Capestone/data/data.zip")
setwd("~/R/Capestone")
}
blog <- read_lines("~/R/Capestone/data/final/en_US/en_US.blogs.txt")
news <- read_lines("~/R/Capestone/data/final/en_US/en_US.news.txt")
twitter <- read_lines("~/R/Capestone/data/final/en_US/en_US.twitter.txt")
blog <- tibble(text = blog)
news <- tibble(text = news)
twitter <- tibble(text = twitter)
set.seed(90210)
corpus <- full_join(blog,twitter) %>% full_join(news) %>%
slice_sample(prop = 0.10)
prof <- read_lines("~/R/Capestone/data/facebook-bad-words-list_comma-separated-text-file_2021_01_18.txt")[15]
prof <- prof %>% str_split(", ") %>% flatten %>% unlist
prof <- tibble("word" = prof)
english <- read_lines("~/R/Capestone/data/diction.txt")
english <- tibble("word" = english[!english==""])
#clean up ram
rm(blog,news,twitter)
voc <- english %>% anti_join(prof)
unigram <- corpus %>% unnest_tokens(ngram, text, token = "ngrams", n = 1) %>%
semi_join(voc, by = c("ngram"="word"))
#decreases the voc size
voc <- tibble(word = unigram$ngram)
#OOV 1% of the least likely unigrams
unk <- unigram %>%
count(ngram) %>%
filter(n==1) %>%
slice_sample(prop = 0.005)
unigram[unigram$ngram %in% unk$ngram,]$ngram <- "<unk>"
unigram <- count(unigram, ngram)
remove <- unigram$n==1
unigram <- unigram[!remove,]
voc <- tibble(word = unigram$ngram)
unigram
bind\
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = TRUE)
library(tidyverse)
library(tidytext)
library(pryr)
#downloads the corpus files, profanity filter and English dictionary
url <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
url2 <- "https://www.freewebheaders.com/download/files/facebook-bad-words-list_comma-separated-text-file_2021_01_18.zip"
url3 <- "https://raw.githubusercontent.com/dwyl/english-words/master/words_alpha.txt"
if(dir.exists("~/R/Capestone/data/") == FALSE){
dir.create("~/R/Capestone/data/")}
if(file.exists("~/R/Capestone/data/data.zip") == FALSE|
file.exists("~/R/Capestone/data/prof.zip")==FALSE|
file.exists("~/R/Capestone/data/diction.txt")==FALSE){
download.file(url,destfile = "~/R/Capestone/data/data.zip")
download.file(url2,destfile = "~/R/Capestone/data/prof.zip")
download.file(url3,destfile = "~/R/Capestone/data/diction.txt")
setwd("~/R/Capestone/data/")
unzip("~/R/Capestone/data/prof.zip")
unzip("~/R/Capestone/data/data.zip")
setwd("~/R/Capestone")
}
blog <- read_lines("~/R/Capestone/data/final/en_US/en_US.blogs.txt")
news <- read_lines("~/R/Capestone/data/final/en_US/en_US.news.txt")
twitter <- read_lines("~/R/Capestone/data/final/en_US/en_US.twitter.txt")
blog <- tibble(text = blog)
news <- tibble(text = news)
twitter <- tibble(text = twitter)
set.seed(90210)
corpus <- full_join(blog,twitter) %>% full_join(news) %>%
slice_sample(prop = 0.10)
bind_rows(blog,twitter,news)
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = TRUE)
library(tidyverse)
library(tidytext)
library(pryr)
#downloads the corpus files, profanity filter and English dictionary
url <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
url2 <- "https://www.freewebheaders.com/download/files/facebook-bad-words-list_comma-separated-text-file_2021_01_18.zip"
url3 <- "https://raw.githubusercontent.com/dwyl/english-words/master/words_alpha.txt"
if(dir.exists("~/R/Capestone/data/") == FALSE){
dir.create("~/R/Capestone/data/")}
if(file.exists("~/R/Capestone/data/data.zip") == FALSE|
file.exists("~/R/Capestone/data/prof.zip")==FALSE|
file.exists("~/R/Capestone/data/diction.txt")==FALSE){
download.file(url,destfile = "~/R/Capestone/data/data.zip")
download.file(url2,destfile = "~/R/Capestone/data/prof.zip")
download.file(url3,destfile = "~/R/Capestone/data/diction.txt")
setwd("~/R/Capestone/data/")
unzip("~/R/Capestone/data/prof.zip")
unzip("~/R/Capestone/data/data.zip")
setwd("~/R/Capestone")
}
blog <- read_lines("~/R/Capestone/data/final/en_US/en_US.blogs.txt")
news <- read_lines("~/R/Capestone/data/final/en_US/en_US.news.txt")
twitter <- read_lines("~/R/Capestone/data/final/en_US/en_US.twitter.txt")
blog <- tibble(text = blog)
news <- tibble(text = news)
twitter <- tibble(text = twitter)
set.seed(90210)
corpus <- bind_rows(blog,twitter,news) %>%
slice_sample(prop = 0.10) %>%
mutate(line = row_number())
corpus
View(corpus)
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = TRUE)
library(tidyverse)
library(tidytext)
library(pryr)
#downloads the corpus files, profanity filter and English dictionary
url <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
url2 <- "https://www.freewebheaders.com/download/files/facebook-bad-words-list_comma-separated-text-file_2021_01_18.zip"
url3 <- "https://raw.githubusercontent.com/dwyl/english-words/master/words_alpha.txt"
if(dir.exists("~/R/Capestone/data/") == FALSE){
dir.create("~/R/Capestone/data/")}
if(file.exists("~/R/Capestone/data/data.zip") == FALSE|
file.exists("~/R/Capestone/data/prof.zip")==FALSE|
file.exists("~/R/Capestone/data/diction.txt")==FALSE){
download.file(url,destfile = "~/R/Capestone/data/data.zip")
download.file(url2,destfile = "~/R/Capestone/data/prof.zip")
download.file(url3,destfile = "~/R/Capestone/data/diction.txt")
setwd("~/R/Capestone/data/")
unzip("~/R/Capestone/data/prof.zip")
unzip("~/R/Capestone/data/data.zip")
setwd("~/R/Capestone")
}
blog <- read_lines("~/R/Capestone/data/final/en_US/en_US.blogs.txt")
news <- read_lines("~/R/Capestone/data/final/en_US/en_US.news.txt")
twitter <- read_lines("~/R/Capestone/data/final/en_US/en_US.twitter.txt")
blog <- tibble(text = blog)
news <- tibble(text = news)
twitter <- tibble(text = twitter)
set.seed(90210)
corpus <- bind_rows(blog,twitter,news) %>%
slice_sample(prop = 0.10) %>%
mutate(line = row_number())
prof <- read_lines("~/R/Capestone/data/facebook-bad-words-list_comma-separated-text-file_2021_01_18.txt")[15]
prof <- prof %>% str_split(", ") %>% flatten %>% unlist
prof <- tibble("word" = prof)
english <- read_lines("~/R/Capestone/data/diction.txt")
english <- tibble("word" = english[!english==""])
#clean up ram
rm(blog,news,twitter)
voc <- english %>% anti_join(prof)
unigram <- corpus %>% unnest_tokens(ngram, text, token = "ngrams", n = 1) %>%
semi_join(voc, by = c("ngram"="word"))
#decreases the voc size
voc <- tibble(word = unigram$ngram)
unigram
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = TRUE)
library(tidyverse)
library(tidytext)
library(pryr)
#downloads the corpus files, profanity filter and English dictionary
url <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
url2 <- "https://www.freewebheaders.com/download/files/facebook-bad-words-list_comma-separated-text-file_2021_01_18.zip"
url3 <- "https://raw.githubusercontent.com/dwyl/english-words/master/words_alpha.txt"
if(dir.exists("~/R/Capestone/data/") == FALSE){
dir.create("~/R/Capestone/data/")}
if(file.exists("~/R/Capestone/data/data.zip") == FALSE|
file.exists("~/R/Capestone/data/prof.zip")==FALSE|
file.exists("~/R/Capestone/data/diction.txt")==FALSE){
download.file(url,destfile = "~/R/Capestone/data/data.zip")
download.file(url2,destfile = "~/R/Capestone/data/prof.zip")
download.file(url3,destfile = "~/R/Capestone/data/diction.txt")
setwd("~/R/Capestone/data/")
unzip("~/R/Capestone/data/prof.zip")
unzip("~/R/Capestone/data/data.zip")
setwd("~/R/Capestone")
}
blog <- read_lines("~/R/Capestone/data/final/en_US/en_US.blogs.txt")
news <- read_lines("~/R/Capestone/data/final/en_US/en_US.news.txt")
twitter <- read_lines("~/R/Capestone/data/final/en_US/en_US.twitter.txt")
blog <- tibble(text = blog)
news <- tibble(text = news)
twitter <- tibble(text = twitter)
set.seed(90210)
corpus <- bind_rows(blog,twitter,news) %>%
slice_sample(prop = 0.10) %>%
mutate(line = row_number())
prof <- read_lines("~/R/Capestone/data/facebook-bad-words-list_comma-separated-text-file_2021_01_18.txt")[15]
prof <- prof %>% str_split(", ") %>% flatten %>% unlist
prof <- tibble("word" = prof)
english <- read_lines("~/R/Capestone/data/diction.txt")
english <- tibble("word" = english[!english==""])
#clean up ram
rm(blog,news,twitter)
voc <- english %>% anti_join(prof)
unigram <- corpus %>% unnest_tokens(ngram, text, token = "ngrams", n = 1) %>%
semi_join(voc, by = c("ngram"="word"))
#decreases the voc size
voc <- tibble(word = unigram$ngram)
#OOV 1% of the least likely unigrams
unk <- unigram %>%
count(ngram) %>%
filter(n==1) %>%
slice_sample(prop = 0.005)
unigram[unigram$ngram %in% unk$ngram,]$ngram <- "<unk>"
unigram <- count(unigram, ngram)
remove <- unigram$n==1
unigram <- unigram[!remove,]
voc <- tibble(word = unigram$ngram)
unigram
#clean up ram
rm(blog,news,twitter)
voc <- english %>% anti_join(prof)
unigram <- corpus %>% unnest_tokens(ngram, text, token = "ngrams", n = 1) %>%
semi_join(voc, by = c("ngram"="word"))
#decreases the voc size
voc <- tibble(word = unigram$ngram)
voc
unigram
unigram %>% count(ngram)
unigram %>% count(ngram, line)
unigram
unlink('Capestone/Main_notebook_cache', recursive = TRUE)
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = TRUE)
library(tidyverse)
library(tidytext)
library(pryr)
#downloads the corpus files, profanity filter and English dictionary
url <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
url2 <- "https://www.freewebheaders.com/download/files/facebook-bad-words-list_comma-separated-text-file_2021_01_18.zip"
url3 <- "https://raw.githubusercontent.com/dwyl/english-words/master/words_alpha.txt"
url4 <- "https://raw.githubusercontent.com/mark-edney/Capestone/1c143b40dd71f0564c3248df2a8638d08af10440/data/contractions.txt"
if(dir.exists("~/R/Capestone/data/") == FALSE){
dir.create("~/R/Capestone/data/")}
if(file.exists("~/R/Capestone/data/data.zip") == FALSE|
file.exists("~/R/Capestone/data/prof.zip")==FALSE|
file.exists("~/R/Capestone/data/diction.txt")==FALSE|
file.exists("~/R/Capestone/data/contractions.txt")==FALSE){
download.file(url,destfile = "~/R/Capestone/data/data.zip")
download.file(url2,destfile = "~/R/Capestone/data/prof.zip")
download.file(url3,destfile = "~/R/Capestone/data/diction.txt")
download.file(url4,destfile = "~/R/Capestone/data/contractions.txt")
setwd("~/R/Capestone/data/")
unzip("~/R/Capestone/data/prof.zip")
unzip("~/R/Capestone/data/data.zip")
setwd("~/R/Capestone")
}
blog <- read_lines("~/R/Capestone/data/final/en_US/en_US.blogs.txt")
news <- read_lines("~/R/Capestone/data/final/en_US/en_US.news.txt")
twitter <- read_lines("~/R/Capestone/data/final/en_US/en_US.twitter.txt")
blog <- tibble(text = blog)
news <- tibble(text = news)
twitter <- tibble(text = twitter)
set.seed(90210)
corpus <- bind_rows(blog,twitter,news) %>%
slice_sample(prop = 0.05) %>%
mutate(line = row_number())
prof <- read_lines("~/R/Capestone/data/facebook-bad-words-list_comma-separated-text-file_2021_01_18.txt")[15]
prof <- prof %>% str_split(", ") %>% flatten %>% unlist
prof <- tibble("word" = prof)
english <- read_lines("~/R/Capestone/data/diction.txt")
english <- tibble("word" = english[!english==""])
contract <- read_lines("~/R/Capestone/data/contractions.txt")
contract <- tibble("word" = contract)
#clean up ram
rm(blog,news,twitter)
voc <- bind_rows(english, contract) %>% anti_join(prof)
rm(contract, english, prof)
unigram <- corpus %>% unnest_tokens(ngram, text, token = "ngrams", n = 1) %>%
semi_join(voc, by = c("ngram"="word"))
#decreases the voc size
voc <- tibble(word = unique(unigram$ngram))
#OOV 1% of the least likely unigrams
unigramcount <- unigram %>% count(ngram)
unk <- unigramcount %>%
filter(n==1) %>%
slice_sample(prop = 0.005)
unigram[unigram$ngram %in% unk$ngram,]$ngram <- "<unk>"
remove <- unigramcount %>%
filter(n==1)
unigram <- unigram[!unigram$ngram %in% remove$ngram,]
voc <- tibble(word = unique(unigram$ngram))
corpus <- unigram %>%
group_by(line) %>%
summarise(line = paste(ngram, collapse = " ")) %>%
rename("text" = "line") %>%
mutate("line" = row_number())
rm(unigramcount)
unigram <- unigram %>% count(ngram)
bigram <- corpus %>%
unnest_tokens(ngram, text, token = "ngrams", n = 2) %>%
separate(ngram, c("word1", "word2")) %>%
count(word1, word2) %>%
filter(!n==1)
trigram <- corpus %>%
unnest_tokens(ngram, text, token = "ngrams", n = 3) %>%
separate(ngram, c("word1", "word2", "word3")) %>%
count(word1, word2, word3) %>%
filter(!n==1)
rm(corpus)
dist <- tibble(word = voc$word)
input <- tibble(text = c("When you breathe, I want to be the air for you. I'll be there for you, I'd live and I'd",
"Guy at my table's wife got up to go to the bathroom and I asked about dessert and he started telling me about his",
"I'd give anything to see arctic monkeys this",
"Talking to your mom has the same effect as a hug and helps reduce your",
"When you were in Holland you were like 1 inch away from me but you hadn't time to take a",
"I'd just like all of these questions answered, a presentation of evidence, and a jury to settle the",
"I can't deal with unsymetrical things. I can't even hold an uneven number of bags of groceries in each",
"Every inch of you is perfect from the bottom to the",
"I’m thankful my childhood was filled with imagination and bruises from playing",
"I like how the same people are in almost all of Adam Sandler's")) %>%
mutate(line = row_number()) %>%
unnest_tokens(word, text)
w <- filter(input, line==1) %>%
select(word) %>%
slice_tail(n=2) %>%
as.vector()
w[!w$word %in% voc$word,] <- "<unk>"
temp <- trigram %>% filter(word1 %in% w[1,], word2 %in% w[2,])
MLEdf <- tibble(word=temp$word3, MLE=temp$n/sum(temp$n))
dist <- left_join(dist, MLEdf)
dist[is.na(dist$MLE),]$MLE <- 0
ADDdf <- tibble(word=temp$word3, ADD=temp$n)
dist <- left_join(dist, ADDdf)
dist[is.na(dist$ADD),]$ADD <- 0
dist$ADD <- dist$ADD + 1
dist$ADD <- dist$ADD / sum(dist$ADD)
Nr <- count(temp, n) %>% add_row(n = 0, nn = 0)  %>%
arrange(n)
Nr %<>% mutate(c= 0) %>% mutate(GT = 0)
total <- sum(Nr$nn*Nr$n)
Nr$GT[Nr$n==0] <- Nr$nn[Nr$n==1] / total
w
voc$word
sum(voc$word=="i'd")
#clean up ram
rm(blog,news,twitter)
voc <- bind_rows(english, contract) %>% anti_join(prof)
prof <- read_lines("~/R/Capestone/data/facebook-bad-words-list_comma-separated-text-file_2021_01_18.txt")[15]
prof <- prof %>% str_split(", ") %>% flatten %>% unlist
prof <- tibble("word" = prof)
english <- read_lines("~/R/Capestone/data/diction.txt")
english <- tibble("word" = english[!english==""])
contract <- read_lines("~/R/Capestone/data/contractions.txt")
contract <- tibble("word" = contract)
#clean up ram
rm(blog,news,twitter)
voc <- bind_rows(english, contract) %>% anti_join(prof)
unigram <- corpus %>% unnest_tokens(ngram, text, token = "ngrams", n = 1) %>%
semi_join(voc, by = c("ngram"="word"))
blog <- read_lines("~/R/Capestone/data/final/en_US/en_US.blogs.txt")
news <- read_lines("~/R/Capestone/data/final/en_US/en_US.news.txt")
twitter <- read_lines("~/R/Capestone/data/final/en_US/en_US.twitter.txt")
blog <- tibble(text = blog)
news <- tibble(text = news)
twitter <- tibble(text = twitter)
set.seed(90210)
corpus <- bind_rows(blog,twitter,news) %>%
slice_sample(prop = 0.05) %>%
mutate(line = row_number())
prof <- read_lines("~/R/Capestone/data/facebook-bad-words-list_comma-separated-text-file_2021_01_18.txt")[15]
prof <- prof %>% str_split(", ") %>% flatten %>% unlist
prof <- tibble("word" = prof)
english <- read_lines("~/R/Capestone/data/diction.txt")
english <- tibble("word" = english[!english==""])
contract <- read_lines("~/R/Capestone/data/contractions.txt")
contract <- tibble("word" = contract)
#clean up ram
rm(blog,news,twitter)
voc <- bind_rows(english, contract) %>% anti_join(prof)
unigram <- corpus %>% unnest_tokens(ngram, text, token = "ngrams", n = 1) %>%
semi_join(voc, by = c("ngram"="word"))
#decreases the voc size
voc <- tibble(word = unique(unigram$ngram))
sum(voc$word=="i'd")
sum(contract$word=="i'd")
contract
View(contract)
contract %>% str_to_lower()
contract <- contract %>% str_to_lower()
prof <- read_lines("~/R/Capestone/data/facebook-bad-words-list_comma-separated-text-file_2021_01_18.txt")[15]
prof <- prof %>% str_split(", ") %>% flatten %>% unlist
prof <- tibble("word" = prof)
english <- read_lines("~/R/Capestone/data/diction.txt")
english <- tibble("word" = english[!english==""])
contract <- read_lines("~/R/Capestone/data/contractions.txt")
contract <- tibble("word" = contract)
contract %>% unnest_tokens(word, word)
prof <- read_lines("~/R/Capestone/data/facebook-bad-words-list_comma-separated-text-file_2021_01_18.txt")[15]
prof <- prof %>% str_split(", ") %>% flatten %>% unlist
prof <- tibble("word" = prof)
english <- read_lines("~/R/Capestone/data/diction.txt")
english <- tibble("word" = english[!english==""])
contract <- read_lines("~/R/Capestone/data/contractions.txt")
contract <- tibble("word" = contract) %>% unnest_tokens(word,word)
contract
#clean up ram
rm(blog,news,twitter)
voc <- bind_rows(english, contract) %>% anti_join(prof)
unigram <- corpus %>% unnest_tokens(ngram, text, token = "ngrams", n = 1) %>%
semi_join(voc, by = c("ngram"="word"))
#decreases the voc size
voc <- tibble(word = unique(unigram$ngram))
sum(voc$word=="i'd")
#OOV 1% of the least likely unigrams
unigramcount <- unigram %>% count(ngram)
unk <- unigramcount %>%
filter(n==1) %>%
slice_sample(prop = 0.005)
unigram[unigram$ngram %in% unk$ngram,]$ngram <- "<unk>"
unigram <- unigram %>%
filter(!n==1)
#clean up ram
rm(blog,news,twitter)
voc <- bind_rows(english, contract) %>% anti_join(prof)
unigram <- corpus %>% unnest_tokens(ngram, text, token = "ngrams", n = 1) %>%
semi_join(voc, by = c("ngram"="word"))
#decreases the voc size
voc <- tibble(word = unique(unigram$ngram))
#OOV 1% of the least likely unigrams
unigramcount <- unigram %>% count(ngram)
unk <- unigramcount %>%
filter(n==1) %>%
slice_sample(prop = 0.005)
unigram[unigram$ngram %in% unk$ngram,]$ngram <- "<unk>"
unigram <- unigram %>%
count(ngram) %>%
filter(!n==1)
voc <- tibble(word = unique(unigram$ngram))
sum(voc$word=="i'd")
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = TRUE)
library(tidyverse)
library(tidytext)
library(pryr)
#downloads the corpus files, profanity filter and English dictionary
url <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
url2 <- "https://www.freewebheaders.com/download/files/facebook-bad-words-list_comma-separated-text-file_2021_01_18.zip"
url3 <- "https://raw.githubusercontent.com/dwyl/english-words/master/words_alpha.txt"
url4 <- "https://raw.githubusercontent.com/mark-edney/Capestone/1c143b40dd71f0564c3248df2a8638d08af10440/data/contractions.txt"
if(dir.exists("~/R/Capestone/data/") == FALSE){
dir.create("~/R/Capestone/data/")}
if(file.exists("~/R/Capestone/data/data.zip") == FALSE|
file.exists("~/R/Capestone/data/prof.zip")==FALSE|
file.exists("~/R/Capestone/data/diction.txt")==FALSE|
file.exists("~/R/Capestone/data/contractions.txt")==FALSE){
download.file(url,destfile = "~/R/Capestone/data/data.zip")
download.file(url2,destfile = "~/R/Capestone/data/prof.zip")
download.file(url3,destfile = "~/R/Capestone/data/diction.txt")
download.file(url4,destfile = "~/R/Capestone/data/contractions.txt")
setwd("~/R/Capestone/data/")
unzip("~/R/Capestone/data/prof.zip")
unzip("~/R/Capestone/data/data.zip")
setwd("~/R/Capestone")
}
blog <- read_lines("~/R/Capestone/data/final/en_US/en_US.blogs.txt")
news <- read_lines("~/R/Capestone/data/final/en_US/en_US.news.txt")
twitter <- read_lines("~/R/Capestone/data/final/en_US/en_US.twitter.txt")
blog <- tibble(text = blog)
news <- tibble(text = news)
twitter <- tibble(text = twitter)
set.seed(90210)
corpus <- bind_rows(blog,twitter,news) %>%
slice_sample(prop = 0.05) %>%
mutate(line = row_number())
prof <- read_lines("~/R/Capestone/data/facebook-bad-words-list_comma-separated-text-file_2021_01_18.txt")[15]
prof <- prof %>% str_split(", ") %>% flatten %>% unlist
prof <- tibble("word" = prof)
english <- read_lines("~/R/Capestone/data/diction.txt")
english <- tibble("word" = english[!english==""])
contract <- read_lines("~/R/Capestone/data/contractions.txt")
contract <- tibble("word" = contract) %>% unnest_tokens(word,word)
#clean up ram
rm(blog,news,twitter)
voc <- bind_rows(english, contract) %>% anti_join(prof)
unigram <- corpus %>% unnest_tokens(ngram, text, token = "ngrams", n = 1) %>%
semi_join(voc, by = c("ngram"="word"))
#decreases the voc size
voc <- tibble(word = unique(unigram$ngram))
#OOV 1% of the least likely unigrams
unigramcount <- unigram %>% count(ngram)
unk <- unigramcount %>%
filter(n==1) %>%
slice_sample(prop = 0.005)
unigram[unigram$ngram %in% unk$ngram,]$ngram <- "<unk>"
unigram <- unigram %>%
count(ngram) %>%
filter(!n==1)
voc <- tibble(word = unique(unigram$ngram))
corpus <- unigram %>%
group_by(line) %>%
summarise(line = paste(ngram, collapse = " ")) %>%
rename("text" = "line") %>%
mutate("line" = row_number())
unlink('Capestone/Main_notebook_cache', recursive = TRUE)
shiny::runApp('Capestone/Shiny App/Capestone')
runApp('Shiny App/Capestone')
runApp('Shiny App/Capestone')
runApp('Capestone/Shiny App/Capestone')
runApp('Shiny App/Capestone')
runApp('Capestone/Shiny App/Capestone')
runApp('Capestone/Shiny App/Capestone')
runApp('Capestone/Shiny App/Capestone')
runApp('Capestone/Shiny App/Capestone')
runApp('Capestone/Shiny App/Capestone')
runApp('Capestone/Shiny App/Capestone')
runApp('Capestone/Shiny App/Capestone')
runApp('Capestone/Shiny App/Capestone')
runApp('Capestone/Shiny App/Capestone')
runApp('Capestone/Shiny App/Capestone')
runApp('Capestone/Shiny App/Capestone')
runApp('Capestone/Shiny App/Capestone')
shiny::runApp('Capestone/Shiny App/Capestone')
shiny::runApp('Capestone/Shiny App/Capestone')
runApp('Capestone/Shiny App/Capestone')
runApp('Capestone/Shiny App/Capestone')
runApp('Capestone/Shiny App/Capestone')
corpus <- readRDS("corpus10.rds")
setwd("~/R/Capestone")
corpus <- readRDS("corpus10.rds")
s <- sample(nrow(corpus), 5)
corpus
corpus[s,1]
