setwd("~/R/Capestone/data/")
unzip("~/R/Capestone/data/prof.zip")
unzip("~/R/Capestone/data/data.zip")
setwd("~/R/Capestone")
}
blog <- read_lines("~/R/Capestone/data/final/en_US/en_US.blogs.txt")
news <- read_lines("~/R/Capestone/data/final/en_US/en_US.news.txt")
twitter <- read_lines("~/R/Capestone/data/final/en_US/en_US.twitter.txt")
blog <- tibble(text = blog)
news <- tibble(text = news)
twitter <- tibble(text = twitter)
set.seed(90210)
corpus <- full_join(blog,twitter) %>% full_join(news) %>%
slice_sample(prop = 0.10)
prof <- read_lines("~/R/Capestone/data/facebook-bad-words-list_comma-separated-text-file_2021_01_18.txt")[15]
prof <- prof %>% str_split(", ") %>% flatten %>% unlist
prof <- tibble("word" = prof)
english <- read_lines("~/R/Capestone/data/diction.txt")
english <- tibble("word" = english[!english==""])
#clean up ram
rm(blog,news,twitter)
voc <- english %>% anti_join(prof)
unigram <- corpus %>% unnest_tokens(ngram, text, token = "ngrams", n = 1) %>%
semi_join(voc, by = c("ngram"="word"))
#decreases the voc size
voc <- tibble(word = unigram$ngram)
#OOV 1% of the least likely unigrams
unk <- unigram %>%
count(ngram) %>%
filter(n==1) %>%
slice_sample(prop = 0.005)
unigram[unigram$ngram %in% unk$ngram,]$ngram <- "<unk>"
unigram <- count(unigram, ngram)
remove <- unigram$n==1
unigram <- unigram[!remove,]
voc <- tibble(word = unigram$ngram)
unigram
bind\
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = TRUE)
library(tidyverse)
library(tidytext)
library(pryr)
#downloads the corpus files, profanity filter and English dictionary
url <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
url2 <- "https://www.freewebheaders.com/download/files/facebook-bad-words-list_comma-separated-text-file_2021_01_18.zip"
url3 <- "https://raw.githubusercontent.com/dwyl/english-words/master/words_alpha.txt"
if(dir.exists("~/R/Capestone/data/") == FALSE){
dir.create("~/R/Capestone/data/")}
if(file.exists("~/R/Capestone/data/data.zip") == FALSE|
file.exists("~/R/Capestone/data/prof.zip")==FALSE|
file.exists("~/R/Capestone/data/diction.txt")==FALSE){
download.file(url,destfile = "~/R/Capestone/data/data.zip")
download.file(url2,destfile = "~/R/Capestone/data/prof.zip")
download.file(url3,destfile = "~/R/Capestone/data/diction.txt")
setwd("~/R/Capestone/data/")
unzip("~/R/Capestone/data/prof.zip")
unzip("~/R/Capestone/data/data.zip")
setwd("~/R/Capestone")
}
blog <- read_lines("~/R/Capestone/data/final/en_US/en_US.blogs.txt")
news <- read_lines("~/R/Capestone/data/final/en_US/en_US.news.txt")
twitter <- read_lines("~/R/Capestone/data/final/en_US/en_US.twitter.txt")
blog <- tibble(text = blog)
news <- tibble(text = news)
twitter <- tibble(text = twitter)
set.seed(90210)
corpus <- full_join(blog,twitter) %>% full_join(news) %>%
slice_sample(prop = 0.10)
bind_rows(blog,twitter,news)
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = TRUE)
library(tidyverse)
library(tidytext)
library(pryr)
#downloads the corpus files, profanity filter and English dictionary
url <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
url2 <- "https://www.freewebheaders.com/download/files/facebook-bad-words-list_comma-separated-text-file_2021_01_18.zip"
url3 <- "https://raw.githubusercontent.com/dwyl/english-words/master/words_alpha.txt"
if(dir.exists("~/R/Capestone/data/") == FALSE){
dir.create("~/R/Capestone/data/")}
if(file.exists("~/R/Capestone/data/data.zip") == FALSE|
file.exists("~/R/Capestone/data/prof.zip")==FALSE|
file.exists("~/R/Capestone/data/diction.txt")==FALSE){
download.file(url,destfile = "~/R/Capestone/data/data.zip")
download.file(url2,destfile = "~/R/Capestone/data/prof.zip")
download.file(url3,destfile = "~/R/Capestone/data/diction.txt")
setwd("~/R/Capestone/data/")
unzip("~/R/Capestone/data/prof.zip")
unzip("~/R/Capestone/data/data.zip")
setwd("~/R/Capestone")
}
blog <- read_lines("~/R/Capestone/data/final/en_US/en_US.blogs.txt")
news <- read_lines("~/R/Capestone/data/final/en_US/en_US.news.txt")
twitter <- read_lines("~/R/Capestone/data/final/en_US/en_US.twitter.txt")
blog <- tibble(text = blog)
news <- tibble(text = news)
twitter <- tibble(text = twitter)
set.seed(90210)
corpus <- bind_rows(blog,twitter,news) %>%
slice_sample(prop = 0.10) %>%
mutate(line = row_number())
corpus
View(corpus)
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = TRUE)
library(tidyverse)
library(tidytext)
library(pryr)
#downloads the corpus files, profanity filter and English dictionary
url <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
url2 <- "https://www.freewebheaders.com/download/files/facebook-bad-words-list_comma-separated-text-file_2021_01_18.zip"
url3 <- "https://raw.githubusercontent.com/dwyl/english-words/master/words_alpha.txt"
if(dir.exists("~/R/Capestone/data/") == FALSE){
dir.create("~/R/Capestone/data/")}
if(file.exists("~/R/Capestone/data/data.zip") == FALSE|
file.exists("~/R/Capestone/data/prof.zip")==FALSE|
file.exists("~/R/Capestone/data/diction.txt")==FALSE){
download.file(url,destfile = "~/R/Capestone/data/data.zip")
download.file(url2,destfile = "~/R/Capestone/data/prof.zip")
download.file(url3,destfile = "~/R/Capestone/data/diction.txt")
setwd("~/R/Capestone/data/")
unzip("~/R/Capestone/data/prof.zip")
unzip("~/R/Capestone/data/data.zip")
setwd("~/R/Capestone")
}
blog <- read_lines("~/R/Capestone/data/final/en_US/en_US.blogs.txt")
news <- read_lines("~/R/Capestone/data/final/en_US/en_US.news.txt")
twitter <- read_lines("~/R/Capestone/data/final/en_US/en_US.twitter.txt")
blog <- tibble(text = blog)
news <- tibble(text = news)
twitter <- tibble(text = twitter)
set.seed(90210)
corpus <- bind_rows(blog,twitter,news) %>%
slice_sample(prop = 0.10) %>%
mutate(line = row_number())
prof <- read_lines("~/R/Capestone/data/facebook-bad-words-list_comma-separated-text-file_2021_01_18.txt")[15]
prof <- prof %>% str_split(", ") %>% flatten %>% unlist
prof <- tibble("word" = prof)
english <- read_lines("~/R/Capestone/data/diction.txt")
english <- tibble("word" = english[!english==""])
#clean up ram
rm(blog,news,twitter)
voc <- english %>% anti_join(prof)
unigram <- corpus %>% unnest_tokens(ngram, text, token = "ngrams", n = 1) %>%
semi_join(voc, by = c("ngram"="word"))
#decreases the voc size
voc <- tibble(word = unigram$ngram)
unigram
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = TRUE)
library(tidyverse)
library(tidytext)
library(pryr)
#downloads the corpus files, profanity filter and English dictionary
url <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
url2 <- "https://www.freewebheaders.com/download/files/facebook-bad-words-list_comma-separated-text-file_2021_01_18.zip"
url3 <- "https://raw.githubusercontent.com/dwyl/english-words/master/words_alpha.txt"
if(dir.exists("~/R/Capestone/data/") == FALSE){
dir.create("~/R/Capestone/data/")}
if(file.exists("~/R/Capestone/data/data.zip") == FALSE|
file.exists("~/R/Capestone/data/prof.zip")==FALSE|
file.exists("~/R/Capestone/data/diction.txt")==FALSE){
download.file(url,destfile = "~/R/Capestone/data/data.zip")
download.file(url2,destfile = "~/R/Capestone/data/prof.zip")
download.file(url3,destfile = "~/R/Capestone/data/diction.txt")
setwd("~/R/Capestone/data/")
unzip("~/R/Capestone/data/prof.zip")
unzip("~/R/Capestone/data/data.zip")
setwd("~/R/Capestone")
}
blog <- read_lines("~/R/Capestone/data/final/en_US/en_US.blogs.txt")
news <- read_lines("~/R/Capestone/data/final/en_US/en_US.news.txt")
twitter <- read_lines("~/R/Capestone/data/final/en_US/en_US.twitter.txt")
blog <- tibble(text = blog)
news <- tibble(text = news)
twitter <- tibble(text = twitter)
set.seed(90210)
corpus <- bind_rows(blog,twitter,news) %>%
slice_sample(prop = 0.10) %>%
mutate(line = row_number())
prof <- read_lines("~/R/Capestone/data/facebook-bad-words-list_comma-separated-text-file_2021_01_18.txt")[15]
prof <- prof %>% str_split(", ") %>% flatten %>% unlist
prof <- tibble("word" = prof)
english <- read_lines("~/R/Capestone/data/diction.txt")
english <- tibble("word" = english[!english==""])
#clean up ram
rm(blog,news,twitter)
voc <- english %>% anti_join(prof)
unigram <- corpus %>% unnest_tokens(ngram, text, token = "ngrams", n = 1) %>%
semi_join(voc, by = c("ngram"="word"))
#decreases the voc size
voc <- tibble(word = unigram$ngram)
#OOV 1% of the least likely unigrams
unk <- unigram %>%
count(ngram) %>%
filter(n==1) %>%
slice_sample(prop = 0.005)
unigram[unigram$ngram %in% unk$ngram,]$ngram <- "<unk>"
unigram <- count(unigram, ngram)
remove <- unigram$n==1
unigram <- unigram[!remove,]
voc <- tibble(word = unigram$ngram)
unigram
#clean up ram
rm(blog,news,twitter)
voc <- english %>% anti_join(prof)
unigram <- corpus %>% unnest_tokens(ngram, text, token = "ngrams", n = 1) %>%
semi_join(voc, by = c("ngram"="word"))
#decreases the voc size
voc <- tibble(word = unigram$ngram)
voc
unigram
unigram %>% count(ngram)
unigram %>% count(ngram, line)
unigram
unlink('Capestone/Main_notebook_cache', recursive = TRUE)
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = TRUE)
library(tidyverse)
library(tidytext)
library(pryr)
#downloads the corpus files, profanity filter and English dictionary
url <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
url2 <- "https://www.freewebheaders.com/download/files/facebook-bad-words-list_comma-separated-text-file_2021_01_18.zip"
url3 <- "https://raw.githubusercontent.com/dwyl/english-words/master/words_alpha.txt"
url4 <- "https://raw.githubusercontent.com/mark-edney/Capestone/1c143b40dd71f0564c3248df2a8638d08af10440/data/contractions.txt"
if(dir.exists("~/R/Capestone/data/") == FALSE){
dir.create("~/R/Capestone/data/")}
if(file.exists("~/R/Capestone/data/data.zip") == FALSE|
file.exists("~/R/Capestone/data/prof.zip")==FALSE|
file.exists("~/R/Capestone/data/diction.txt")==FALSE|
file.exists("~/R/Capestone/data/contractions.txt")==FALSE){
download.file(url,destfile = "~/R/Capestone/data/data.zip")
download.file(url2,destfile = "~/R/Capestone/data/prof.zip")
download.file(url3,destfile = "~/R/Capestone/data/diction.txt")
download.file(url4,destfile = "~/R/Capestone/data/contractions.txt")
setwd("~/R/Capestone/data/")
unzip("~/R/Capestone/data/prof.zip")
unzip("~/R/Capestone/data/data.zip")
setwd("~/R/Capestone")
}
blog <- read_lines("~/R/Capestone/data/final/en_US/en_US.blogs.txt")
news <- read_lines("~/R/Capestone/data/final/en_US/en_US.news.txt")
twitter <- read_lines("~/R/Capestone/data/final/en_US/en_US.twitter.txt")
blog <- tibble(text = blog)
news <- tibble(text = news)
twitter <- tibble(text = twitter)
set.seed(90210)
corpus <- bind_rows(blog,twitter,news) %>%
slice_sample(prop = 0.05) %>%
mutate(line = row_number())
prof <- read_lines("~/R/Capestone/data/facebook-bad-words-list_comma-separated-text-file_2021_01_18.txt")[15]
prof <- prof %>% str_split(", ") %>% flatten %>% unlist
prof <- tibble("word" = prof)
english <- read_lines("~/R/Capestone/data/diction.txt")
english <- tibble("word" = english[!english==""])
contract <- read_lines("~/R/Capestone/data/contractions.txt")
contract <- tibble("word" = contract)
#clean up ram
rm(blog,news,twitter)
voc <- bind_rows(english, contract) %>% anti_join(prof)
rm(contract, english, prof)
unigram <- corpus %>% unnest_tokens(ngram, text, token = "ngrams", n = 1) %>%
semi_join(voc, by = c("ngram"="word"))
#decreases the voc size
voc <- tibble(word = unique(unigram$ngram))
#OOV 1% of the least likely unigrams
unigramcount <- unigram %>% count(ngram)
unk <- unigramcount %>%
filter(n==1) %>%
slice_sample(prop = 0.005)
unigram[unigram$ngram %in% unk$ngram,]$ngram <- "<unk>"
remove <- unigramcount %>%
filter(n==1)
unigram <- unigram[!unigram$ngram %in% remove$ngram,]
voc <- tibble(word = unique(unigram$ngram))
corpus <- unigram %>%
group_by(line) %>%
summarise(line = paste(ngram, collapse = " ")) %>%
rename("text" = "line") %>%
mutate("line" = row_number())
rm(unigramcount)
unigram <- unigram %>% count(ngram)
bigram <- corpus %>%
unnest_tokens(ngram, text, token = "ngrams", n = 2) %>%
separate(ngram, c("word1", "word2")) %>%
count(word1, word2) %>%
filter(!n==1)
trigram <- corpus %>%
unnest_tokens(ngram, text, token = "ngrams", n = 3) %>%
separate(ngram, c("word1", "word2", "word3")) %>%
count(word1, word2, word3) %>%
filter(!n==1)
rm(corpus)
dist <- tibble(word = voc$word)
input <- tibble(text = c("When you breathe, I want to be the air for you. I'll be there for you, I'd live and I'd",
"Guy at my table's wife got up to go to the bathroom and I asked about dessert and he started telling me about his",
"I'd give anything to see arctic monkeys this",
"Talking to your mom has the same effect as a hug and helps reduce your",
"When you were in Holland you were like 1 inch away from me but you hadn't time to take a",
"I'd just like all of these questions answered, a presentation of evidence, and a jury to settle the",
"I can't deal with unsymetrical things. I can't even hold an uneven number of bags of groceries in each",
"Every inch of you is perfect from the bottom to the",
"Iâ€™m thankful my childhood was filled with imagination and bruises from playing",
"I like how the same people are in almost all of Adam Sandler's")) %>%
mutate(line = row_number()) %>%
unnest_tokens(word, text)
w <- filter(input, line==1) %>%
select(word) %>%
slice_tail(n=2) %>%
as.vector()
w[!w$word %in% voc$word,] <- "<unk>"
temp <- trigram %>% filter(word1 %in% w[1,], word2 %in% w[2,])
MLEdf <- tibble(word=temp$word3, MLE=temp$n/sum(temp$n))
dist <- left_join(dist, MLEdf)
dist[is.na(dist$MLE),]$MLE <- 0
ADDdf <- tibble(word=temp$word3, ADD=temp$n)
dist <- left_join(dist, ADDdf)
dist[is.na(dist$ADD),]$ADD <- 0
dist$ADD <- dist$ADD + 1
dist$ADD <- dist$ADD / sum(dist$ADD)
Nr <- count(temp, n) %>% add_row(n = 0, nn = 0)  %>%
arrange(n)
Nr %<>% mutate(c= 0) %>% mutate(GT = 0)
total <- sum(Nr$nn*Nr$n)
Nr$GT[Nr$n==0] <- Nr$nn[Nr$n==1] / total
w
voc$word
sum(voc$word=="i'd")
#clean up ram
rm(blog,news,twitter)
voc <- bind_rows(english, contract) %>% anti_join(prof)
prof <- read_lines("~/R/Capestone/data/facebook-bad-words-list_comma-separated-text-file_2021_01_18.txt")[15]
prof <- prof %>% str_split(", ") %>% flatten %>% unlist
prof <- tibble("word" = prof)
english <- read_lines("~/R/Capestone/data/diction.txt")
english <- tibble("word" = english[!english==""])
contract <- read_lines("~/R/Capestone/data/contractions.txt")
contract <- tibble("word" = contract)
#clean up ram
rm(blog,news,twitter)
voc <- bind_rows(english, contract) %>% anti_join(prof)
unigram <- corpus %>% unnest_tokens(ngram, text, token = "ngrams", n = 1) %>%
semi_join(voc, by = c("ngram"="word"))
blog <- read_lines("~/R/Capestone/data/final/en_US/en_US.blogs.txt")
news <- read_lines("~/R/Capestone/data/final/en_US/en_US.news.txt")
twitter <- read_lines("~/R/Capestone/data/final/en_US/en_US.twitter.txt")
blog <- tibble(text = blog)
news <- tibble(text = news)
twitter <- tibble(text = twitter)
set.seed(90210)
corpus <- bind_rows(blog,twitter,news) %>%
slice_sample(prop = 0.05) %>%
mutate(line = row_number())
prof <- read_lines("~/R/Capestone/data/facebook-bad-words-list_comma-separated-text-file_2021_01_18.txt")[15]
prof <- prof %>% str_split(", ") %>% flatten %>% unlist
prof <- tibble("word" = prof)
english <- read_lines("~/R/Capestone/data/diction.txt")
english <- tibble("word" = english[!english==""])
contract <- read_lines("~/R/Capestone/data/contractions.txt")
contract <- tibble("word" = contract)
#clean up ram
rm(blog,news,twitter)
voc <- bind_rows(english, contract) %>% anti_join(prof)
unigram <- corpus %>% unnest_tokens(ngram, text, token = "ngrams", n = 1) %>%
semi_join(voc, by = c("ngram"="word"))
#decreases the voc size
voc <- tibble(word = unique(unigram$ngram))
sum(voc$word=="i'd")
sum(contract$word=="i'd")
contract
View(contract)
contract %>% str_to_lower()
contract <- contract %>% str_to_lower()
prof <- read_lines("~/R/Capestone/data/facebook-bad-words-list_comma-separated-text-file_2021_01_18.txt")[15]
prof <- prof %>% str_split(", ") %>% flatten %>% unlist
prof <- tibble("word" = prof)
english <- read_lines("~/R/Capestone/data/diction.txt")
english <- tibble("word" = english[!english==""])
contract <- read_lines("~/R/Capestone/data/contractions.txt")
contract <- tibble("word" = contract)
contract %>% unnest_tokens(word, word)
prof <- read_lines("~/R/Capestone/data/facebook-bad-words-list_comma-separated-text-file_2021_01_18.txt")[15]
prof <- prof %>% str_split(", ") %>% flatten %>% unlist
prof <- tibble("word" = prof)
english <- read_lines("~/R/Capestone/data/diction.txt")
english <- tibble("word" = english[!english==""])
contract <- read_lines("~/R/Capestone/data/contractions.txt")
contract <- tibble("word" = contract) %>% unnest_tokens(word,word)
contract
#clean up ram
rm(blog,news,twitter)
voc <- bind_rows(english, contract) %>% anti_join(prof)
unigram <- corpus %>% unnest_tokens(ngram, text, token = "ngrams", n = 1) %>%
semi_join(voc, by = c("ngram"="word"))
#decreases the voc size
voc <- tibble(word = unique(unigram$ngram))
sum(voc$word=="i'd")
#OOV 1% of the least likely unigrams
unigramcount <- unigram %>% count(ngram)
unk <- unigramcount %>%
filter(n==1) %>%
slice_sample(prop = 0.005)
unigram[unigram$ngram %in% unk$ngram,]$ngram <- "<unk>"
unigram <- unigram %>%
filter(!n==1)
#clean up ram
rm(blog,news,twitter)
voc <- bind_rows(english, contract) %>% anti_join(prof)
unigram <- corpus %>% unnest_tokens(ngram, text, token = "ngrams", n = 1) %>%
semi_join(voc, by = c("ngram"="word"))
#decreases the voc size
voc <- tibble(word = unique(unigram$ngram))
#OOV 1% of the least likely unigrams
unigramcount <- unigram %>% count(ngram)
unk <- unigramcount %>%
filter(n==1) %>%
slice_sample(prop = 0.005)
unigram[unigram$ngram %in% unk$ngram,]$ngram <- "<unk>"
unigram <- unigram %>%
count(ngram) %>%
filter(!n==1)
voc <- tibble(word = unique(unigram$ngram))
sum(voc$word=="i'd")
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = TRUE)
library(tidyverse)
library(tidytext)
library(pryr)
#downloads the corpus files, profanity filter and English dictionary
url <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
url2 <- "https://www.freewebheaders.com/download/files/facebook-bad-words-list_comma-separated-text-file_2021_01_18.zip"
url3 <- "https://raw.githubusercontent.com/dwyl/english-words/master/words_alpha.txt"
url4 <- "https://raw.githubusercontent.com/mark-edney/Capestone/1c143b40dd71f0564c3248df2a8638d08af10440/data/contractions.txt"
if(dir.exists("~/R/Capestone/data/") == FALSE){
dir.create("~/R/Capestone/data/")}
if(file.exists("~/R/Capestone/data/data.zip") == FALSE|
file.exists("~/R/Capestone/data/prof.zip")==FALSE|
file.exists("~/R/Capestone/data/diction.txt")==FALSE|
file.exists("~/R/Capestone/data/contractions.txt")==FALSE){
download.file(url,destfile = "~/R/Capestone/data/data.zip")
download.file(url2,destfile = "~/R/Capestone/data/prof.zip")
download.file(url3,destfile = "~/R/Capestone/data/diction.txt")
download.file(url4,destfile = "~/R/Capestone/data/contractions.txt")
setwd("~/R/Capestone/data/")
unzip("~/R/Capestone/data/prof.zip")
unzip("~/R/Capestone/data/data.zip")
setwd("~/R/Capestone")
}
blog <- read_lines("~/R/Capestone/data/final/en_US/en_US.blogs.txt")
news <- read_lines("~/R/Capestone/data/final/en_US/en_US.news.txt")
twitter <- read_lines("~/R/Capestone/data/final/en_US/en_US.twitter.txt")
blog <- tibble(text = blog)
news <- tibble(text = news)
twitter <- tibble(text = twitter)
set.seed(90210)
corpus <- bind_rows(blog,twitter,news) %>%
slice_sample(prop = 0.05) %>%
mutate(line = row_number())
prof <- read_lines("~/R/Capestone/data/facebook-bad-words-list_comma-separated-text-file_2021_01_18.txt")[15]
prof <- prof %>% str_split(", ") %>% flatten %>% unlist
prof <- tibble("word" = prof)
english <- read_lines("~/R/Capestone/data/diction.txt")
english <- tibble("word" = english[!english==""])
contract <- read_lines("~/R/Capestone/data/contractions.txt")
contract <- tibble("word" = contract) %>% unnest_tokens(word,word)
#clean up ram
rm(blog,news,twitter)
voc <- bind_rows(english, contract) %>% anti_join(prof)
unigram <- corpus %>% unnest_tokens(ngram, text, token = "ngrams", n = 1) %>%
semi_join(voc, by = c("ngram"="word"))
#decreases the voc size
voc <- tibble(word = unique(unigram$ngram))
#OOV 1% of the least likely unigrams
unigramcount <- unigram %>% count(ngram)
unk <- unigramcount %>%
filter(n==1) %>%
slice_sample(prop = 0.005)
unigram[unigram$ngram %in% unk$ngram,]$ngram <- "<unk>"
unigram <- unigram %>%
count(ngram) %>%
filter(!n==1)
voc <- tibble(word = unique(unigram$ngram))
corpus <- unigram %>%
group_by(line) %>%
summarise(line = paste(ngram, collapse = " ")) %>%
rename("text" = "line") %>%
mutate("line" = row_number())
unlink('Capestone/Main_notebook_cache', recursive = TRUE)
shiny::runApp('Capestone/Shiny App/Capestone')
runApp('Shiny App/Capestone')
runApp('Shiny App/Capestone')
runApp('Capestone/Shiny App/Capestone')
runApp('Shiny App/Capestone')
runApp('Capestone/Shiny App/Capestone')
runApp('Capestone/Shiny App/Capestone')
runApp('Capestone/Shiny App/Capestone')
runApp('Capestone/Shiny App/Capestone')
runApp('Capestone/Shiny App/Capestone')
runApp('Capestone/Shiny App/Capestone')
runApp('Capestone/Shiny App/Capestone')
runApp('Capestone/Shiny App/Capestone')
runApp('Capestone/Shiny App/Capestone')
runApp('Capestone/Shiny App/Capestone')
runApp('Capestone/Shiny App/Capestone')
runApp('Capestone/Shiny App/Capestone')
shiny::runApp('Capestone/Shiny App/Capestone')
shiny::runApp('Capestone/Shiny App/Capestone')
runApp('Capestone/Shiny App/Capestone')
runApp('Capestone/Shiny App/Capestone')
runApp('Capestone/Shiny App/Capestone')
corpus <- readRDS("corpus10.rds")
setwd("~/R/Capestone")
corpus <- readRDS("corpus10.rds")
s <- sample(nrow(corpus), 5)
corpus
corpus[s,1]
