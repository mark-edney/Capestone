voc <- tibble(word = unigram$ngram)
unk <- unigram %>%
filter(n==1) %>%
slice_sample(prop = 0.1)
unk <- unigram %>%
filter(n==1)
unigram %>%
filter(n==1)
unigram
unigram
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = TRUE)
library(tidyverse)
library(tidytext)
library(stopwords)
#downloads the corpus files, profanity filter and English dictionary
url <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
url2 <- "https://www.freewebheaders.com/download/files/facebook-bad-words-list_comma-separated-text-file_2021_01_18.zip"
url3 <- "https://raw.githubusercontent.com/dwyl/english-words/master/words_alpha.txt"
if(dir.exists("~/R/Capestone/data/") == FALSE){
dir.create("~/R/Capestone/data/")}
if(file.exists("~/R/Capestone/data/data.zip") == FALSE|
file.exists("~/R/Capestone/data/prof.zip")==FALSE|
file.exists("~/R/Capestone/data/diction.txt")==FALSE){
download.file(url,destfile = "~/R/Capestone/data/data.zip")
download.file(url2,destfile = "~/R/Capestone/data/prof.zip")
download.file(url3,destfile = "~/R/Capestone/data/diction.txt")
setwd("~/R/Capestone/data/")
unzip("~/R/Capestone/data/prof.zip")
unzip("~/R/Capestone/data/data.zip")
setwd("~/R/Capestone")
}
blog <- read_lines("~/R/Capestone/data/final/en_US/en_US.blogs.txt")
news <- read_lines("~/R/Capestone/data/final/en_US/en_US.news.txt")
twitter <- read_lines("~/R/Capestone/data/final/en_US/en_US.twitter.txt")
blog <- tibble(text = blog)
news <- tibble(text = news)
twitter <- tibble(text = twitter)
set.seed(90210)
corpus <- full_join(blog,twitter) %>% full_join(news) %>%
slice_sample(prop = 0.10)
prof <- read_lines("~/R/Capestone/data/facebook-bad-words-list_comma-separated-text-file_2021_01_18.txt")[15]
prof <- prof %>% str_split(", ") %>% flatten %>% unlist
prof <- tibble("word" = prof)
english <- read_lines("~/R/Capestone/data/diction.txt")
english <- tibble("word" = english[!english==""])
#clean up ram
rm(blog,news,twitter)
voc <- english %>% anti_join(prof) %>% anti_join(stop_words)
unigram <- corpus %>% unnest_tokens(ngram, text, token = "ngrams", n = 1) %>%
semi_join(voc, by = c("ngram"="word")) %>% count(ngram)
#decreases the voc size
voc <- tibble(word = unigram$ngram)
View(unigram)
unk <- unigram %>%
filter(n==1) %>%
slice_sample(prop = 0.1)
unk
unk <- unigram %>%
filter(n==1) %>%
slice_sample(prop = 0.1) %>%
select(ngram)
unk
unigram$ngram %in% unk
sum(unigram$ngram %in% unk)
unk
sum(unigram %in% unk)
unk <- unigram %>%
filter(n==1) %>%
slice_sample(prop = 0.1)
unk
sum(unigram %in% unk)
sum(unigram$ngram %in% unk)
View(unigram)
View(unk)
sum(unigram$ngram %in% unk$ngram)
#OOV 1% of the least likely unigrams
unk <- unigram %>%
filter(n==1) %>%
slice_sample(prop = 0.1)
unigram[unigram$ngram %in% unk$ngram]$ngram <- "<unk>"
unigram[unigram$ngram %in% unk$ngram]$ngram <- "<unk>"
unigram[unigram$ngram %in% unk$ngram]$ngram
unigram[unigram$ngram %in% unk$ngram]
unigram[unigram$ngram %in% unk$ngram,]
unigram[unigram$ngram %in% unk$ngram]$ngram
unigram[unigram$ngram %in% unk$ngram,]$ngram
#OOV 1% of the least likely unigrams
unk <- unigram %>%
filter(n==1) %>%
slice_sample(prop = 0.1)
unigram[unigram$ngram %in% unk$ngram,]$ngram <- "<unk>"
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = TRUE)
library(tidyverse)
library(tidytext)
library(stopwords)
#downloads the corpus files, profanity filter and English dictionary
url <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
url2 <- "https://www.freewebheaders.com/download/files/facebook-bad-words-list_comma-separated-text-file_2021_01_18.zip"
url3 <- "https://raw.githubusercontent.com/dwyl/english-words/master/words_alpha.txt"
if(dir.exists("~/R/Capestone/data/") == FALSE){
dir.create("~/R/Capestone/data/")}
if(file.exists("~/R/Capestone/data/data.zip") == FALSE|
file.exists("~/R/Capestone/data/prof.zip")==FALSE|
file.exists("~/R/Capestone/data/diction.txt")==FALSE){
download.file(url,destfile = "~/R/Capestone/data/data.zip")
download.file(url2,destfile = "~/R/Capestone/data/prof.zip")
download.file(url3,destfile = "~/R/Capestone/data/diction.txt")
setwd("~/R/Capestone/data/")
unzip("~/R/Capestone/data/prof.zip")
unzip("~/R/Capestone/data/data.zip")
setwd("~/R/Capestone")
}
blog <- read_lines("~/R/Capestone/data/final/en_US/en_US.blogs.txt")
news <- read_lines("~/R/Capestone/data/final/en_US/en_US.news.txt")
twitter <- read_lines("~/R/Capestone/data/final/en_US/en_US.twitter.txt")
blog <- tibble(text = blog)
news <- tibble(text = news)
twitter <- tibble(text = twitter)
set.seed(90210)
corpus <- full_join(blog,twitter) %>% full_join(news) %>%
slice_sample(prop = 0.10)
prof <- read_lines("~/R/Capestone/data/facebook-bad-words-list_comma-separated-text-file_2021_01_18.txt")[15]
prof <- prof %>% str_split(", ") %>% flatten %>% unlist
prof <- tibble("word" = prof)
english <- read_lines("~/R/Capestone/data/diction.txt")
english <- tibble("word" = english[!english==""])
#clean up ram
rm(blog,news,twitter)
voc <- english %>% anti_join(prof) %>% anti_join(stop_words)
unigram <- corpus %>% unnest_tokens(ngram, text, token = "ngrams", n = 1) %>%
semi_join(voc, by = c("ngram"="word"))
#decreases the voc size
voc <- tibble(word = unigram$ngram)
#OOV 1% of the least likely unigrams
unk <- unigram %>%
unique %>%
slice_sample(prop = 0.1)
unigram[unigram$ngram %in% unk$ngram,]$ngram <- "<unk>"
unigram <- count(unigram, ngram)
unigram
max(unigram$n)
summary(unigram)
unk
unk
ungram
unigram
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = TRUE)
library(tidyverse)
library(tidytext)
library(stopwords)
#downloads the corpus files, profanity filter and English dictionary
url <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
url2 <- "https://www.freewebheaders.com/download/files/facebook-bad-words-list_comma-separated-text-file_2021_01_18.zip"
url3 <- "https://raw.githubusercontent.com/dwyl/english-words/master/words_alpha.txt"
if(dir.exists("~/R/Capestone/data/") == FALSE){
dir.create("~/R/Capestone/data/")}
if(file.exists("~/R/Capestone/data/data.zip") == FALSE|
file.exists("~/R/Capestone/data/prof.zip")==FALSE|
file.exists("~/R/Capestone/data/diction.txt")==FALSE){
download.file(url,destfile = "~/R/Capestone/data/data.zip")
download.file(url2,destfile = "~/R/Capestone/data/prof.zip")
download.file(url3,destfile = "~/R/Capestone/data/diction.txt")
setwd("~/R/Capestone/data/")
unzip("~/R/Capestone/data/prof.zip")
unzip("~/R/Capestone/data/data.zip")
setwd("~/R/Capestone")
}
blog <- read_lines("~/R/Capestone/data/final/en_US/en_US.blogs.txt")
news <- read_lines("~/R/Capestone/data/final/en_US/en_US.news.txt")
twitter <- read_lines("~/R/Capestone/data/final/en_US/en_US.twitter.txt")
blog <- tibble(text = blog)
news <- tibble(text = news)
twitter <- tibble(text = twitter)
set.seed(90210)
corpus <- full_join(blog,twitter) %>% full_join(news) %>%
slice_sample(prop = 0.10)
prof <- read_lines("~/R/Capestone/data/facebook-bad-words-list_comma-separated-text-file_2021_01_18.txt")[15]
prof <- prof %>% str_split(", ") %>% flatten %>% unlist
prof <- tibble("word" = prof)
english <- read_lines("~/R/Capestone/data/diction.txt")
english <- tibble("word" = english[!english==""])
#clean up ram
rm(blog,news,twitter)
voc <- english %>% anti_join(prof) %>% anti_join(stop_words)
unigram <- corpus %>% unnest_tokens(ngram, text, token = "ngrams", n = 1) %>%
semi_join(voc, by = c("ngram"="word"))
#decreases the voc size
voc <- tibble(word = unigram$ngram)
unk <- unigram %>%
unique %>%
slice_sample(prop = 0.1)
unk
unigram
sum(unigram==unk)
sum(unigram=unk)
sum(unigram$ngram %in% unk$ngram)
unk <- unigram %>%
select(ngram) %>%
unique %>%
slice_sample(prop = 0.1)
unk
unk <- unigram %>%
select(ngram) %>%
unique %>%
slice_sample(prop = 0.1)
unk <- unigram %>%
unique %>%
slice_sample(prop = 0.1)
unk <- unigram %>%
slice_sample(prop = 0.1)
unk
unk <- unigram %>%
unique
unigram
unigram %>%unique()
unigram %>%unique() %>% slice_sample(prop=.01)
unk <- unigram %>%
select(ngram) %>%
unique %>%
slice_sample(prop = 0.01)
unigram$ngram %in% unk$ngram
sum(unigram$ngram %in% unk$ngram)
unk
sum(unigram$ngram =="piloting")
sum(unigram$ngram =="novus")
unigram[unigram$ngram == "novus"]
unigram[unigram$ngram == "novus",]
unk <- unigram %>%
select(ngram) %>%
unique %>%
slice_sample(prop = 0.01) unk <- unigram %>%
filter(n()==1) %>%
slice_sample(prop = 0.01)
unk <- unigram %>%
filter(n()==1) %>%
slice_sample(prop = 0.01)
unk
unk <- unigram %>%
filter(n()==1)
unk
unigram %>%
filter(n()==1)
unigram
unigram %>%
count(ngram) %>% filter(n()==1)
unigram %>%
count(ngram) %>% filter(n==1
)
unk <- unigram %>%
count(ngram) %>%
filter(n==1) %>%
slice_sample(prop = 0.01)
unk
unigram[unigram$ngram %in% unk$ngram,]$ngram <- "<unk>"
unigram <- count(unigram, ngram)
unigram
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = TRUE)
library(tidyverse)
library(tidytext)
library(stopwords)
#downloads the corpus files, profanity filter and English dictionary
url <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
url2 <- "https://www.freewebheaders.com/download/files/facebook-bad-words-list_comma-separated-text-file_2021_01_18.zip"
url3 <- "https://raw.githubusercontent.com/dwyl/english-words/master/words_alpha.txt"
if(dir.exists("~/R/Capestone/data/") == FALSE){
dir.create("~/R/Capestone/data/")}
if(file.exists("~/R/Capestone/data/data.zip") == FALSE|
file.exists("~/R/Capestone/data/prof.zip")==FALSE|
file.exists("~/R/Capestone/data/diction.txt")==FALSE){
download.file(url,destfile = "~/R/Capestone/data/data.zip")
download.file(url2,destfile = "~/R/Capestone/data/prof.zip")
download.file(url3,destfile = "~/R/Capestone/data/diction.txt")
setwd("~/R/Capestone/data/")
unzip("~/R/Capestone/data/prof.zip")
unzip("~/R/Capestone/data/data.zip")
setwd("~/R/Capestone")
}
blog <- read_lines("~/R/Capestone/data/final/en_US/en_US.blogs.txt")
news <- read_lines("~/R/Capestone/data/final/en_US/en_US.news.txt")
twitter <- read_lines("~/R/Capestone/data/final/en_US/en_US.twitter.txt")
blog <- tibble(text = blog)
news <- tibble(text = news)
twitter <- tibble(text = twitter)
set.seed(90210)
corpus <- full_join(blog,twitter) %>% full_join(news) %>%
slice_sample(prop = 0.10)
prof <- read_lines("~/R/Capestone/data/facebook-bad-words-list_comma-separated-text-file_2021_01_18.txt")[15]
prof <- prof %>% str_split(", ") %>% flatten %>% unlist
prof <- tibble("word" = prof)
english <- read_lines("~/R/Capestone/data/diction.txt")
english <- tibble("word" = english[!english==""])
#clean up ram
rm(blog,news,twitter)
voc <- english %>% anti_join(prof) %>% anti_join(stop_words)
unigram <- corpus %>% unnest_tokens(ngram, text, token = "ngrams", n = 1) %>%
semi_join(voc, by = c("ngram"="word"))
#decreases the voc size
voc <- tibble(word = unigram$ngram)
#OOV 1% of the least likely unigrams
unk <- unigram %>%
count(ngram) %>%
filter(n==1) %>%
slice_sample(prop = 0.005)
unigram[unigram$ngram %in% unk$ngram,]$ngram <- "<unk>"
unigram <- count(unigram, ngram)
unigram
unk
remove <- unigram[unigram$n==1,]
unigram <- unigram[-remove,]
remove <- unigram[unigram$n==1,]
unigram <- unigram[-remove]
remove <- unigram[unigram$n==1,]
remove
unigram[unigram$n==1,]
unigram[unigram$n=1,]
unigram$n==1
remove <- unigram$n==1
unigram,
unigram
unigram <- unigram[-remove]
unigram
unigram <- unigram[-remove,]
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = TRUE)
library(tidyverse)
library(tidytext)
library(stopwords)
#downloads the corpus files, profanity filter and English dictionary
url <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
url2 <- "https://www.freewebheaders.com/download/files/facebook-bad-words-list_comma-separated-text-file_2021_01_18.zip"
url3 <- "https://raw.githubusercontent.com/dwyl/english-words/master/words_alpha.txt"
if(dir.exists("~/R/Capestone/data/") == FALSE){
dir.create("~/R/Capestone/data/")}
if(file.exists("~/R/Capestone/data/data.zip") == FALSE|
file.exists("~/R/Capestone/data/prof.zip")==FALSE|
file.exists("~/R/Capestone/data/diction.txt")==FALSE){
download.file(url,destfile = "~/R/Capestone/data/data.zip")
download.file(url2,destfile = "~/R/Capestone/data/prof.zip")
download.file(url3,destfile = "~/R/Capestone/data/diction.txt")
setwd("~/R/Capestone/data/")
unzip("~/R/Capestone/data/prof.zip")
unzip("~/R/Capestone/data/data.zip")
setwd("~/R/Capestone")
}
blog <- read_lines("~/R/Capestone/data/final/en_US/en_US.blogs.txt")
news <- read_lines("~/R/Capestone/data/final/en_US/en_US.news.txt")
twitter <- read_lines("~/R/Capestone/data/final/en_US/en_US.twitter.txt")
blog <- tibble(text = blog)
news <- tibble(text = news)
twitter <- tibble(text = twitter)
set.seed(90210)
corpus <- full_join(blog,twitter) %>% full_join(news) %>%
slice_sample(prop = 0.10)
prof <- read_lines("~/R/Capestone/data/facebook-bad-words-list_comma-separated-text-file_2021_01_18.txt")[15]
prof <- prof %>% str_split(", ") %>% flatten %>% unlist
prof <- tibble("word" = prof)
english <- read_lines("~/R/Capestone/data/diction.txt")
english <- tibble("word" = english[!english==""])
#clean up ram
rm(blog,news,twitter)
voc <- english %>% anti_join(prof) %>% anti_join(stop_words)
unigram <- corpus %>% unnest_tokens(ngram, text, token = "ngrams", n = 1) %>%
semi_join(voc, by = c("ngram"="word"))
#decreases the voc size
voc <- tibble(word = unigram$ngram)
#OOV 1% of the least likely unigrams
unk <- unigram %>%
count(ngram) %>%
filter(n==1) %>%
slice_sample(prop = 0.005)
unigram[unigram$ngram %in% unk$ngram,]$ngram <- "<unk>"
unigram <- count(unigram, ngram)
remove <- unigram$n==1
unigram <- unigram[-remove,]
voc <- tibble(word = unigram$ngram)
unigram
remove
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = TRUE)
library(tidyverse)
library(tidytext)
library(stopwords)
#downloads the corpus files, profanity filter and English dictionary
url <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
url2 <- "https://www.freewebheaders.com/download/files/facebook-bad-words-list_comma-separated-text-file_2021_01_18.zip"
url3 <- "https://raw.githubusercontent.com/dwyl/english-words/master/words_alpha.txt"
if(dir.exists("~/R/Capestone/data/") == FALSE){
dir.create("~/R/Capestone/data/")}
if(file.exists("~/R/Capestone/data/data.zip") == FALSE|
file.exists("~/R/Capestone/data/prof.zip")==FALSE|
file.exists("~/R/Capestone/data/diction.txt")==FALSE){
download.file(url,destfile = "~/R/Capestone/data/data.zip")
download.file(url2,destfile = "~/R/Capestone/data/prof.zip")
download.file(url3,destfile = "~/R/Capestone/data/diction.txt")
setwd("~/R/Capestone/data/")
unzip("~/R/Capestone/data/prof.zip")
unzip("~/R/Capestone/data/data.zip")
setwd("~/R/Capestone")
}
blog <- read_lines("~/R/Capestone/data/final/en_US/en_US.blogs.txt")
news <- read_lines("~/R/Capestone/data/final/en_US/en_US.news.txt")
twitter <- read_lines("~/R/Capestone/data/final/en_US/en_US.twitter.txt")
blog <- tibble(text = blog)
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = TRUE)
library(tidyverse)
library(tidytext)
library(stopwords)
#downloads the corpus files, profanity filter and English dictionary
url <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
url2 <- "https://www.freewebheaders.com/download/files/facebook-bad-words-list_comma-separated-text-file_2021_01_18.zip"
url3 <- "https://raw.githubusercontent.com/dwyl/english-words/master/words_alpha.txt"
if(dir.exists("~/R/Capestone/data/") == FALSE){
dir.create("~/R/Capestone/data/")}
if(file.exists("~/R/Capestone/data/data.zip") == FALSE|
file.exists("~/R/Capestone/data/prof.zip")==FALSE|
file.exists("~/R/Capestone/data/diction.txt")==FALSE){
download.file(url,destfile = "~/R/Capestone/data/data.zip")
download.file(url2,destfile = "~/R/Capestone/data/prof.zip")
download.file(url3,destfile = "~/R/Capestone/data/diction.txt")
setwd("~/R/Capestone/data/")
unzip("~/R/Capestone/data/prof.zip")
unzip("~/R/Capestone/data/data.zip")
setwd("~/R/Capestone")
}
blog <- read_lines("~/R/Capestone/data/final/en_US/en_US.blogs.txt")
news <- read_lines("~/R/Capestone/data/final/en_US/en_US.news.txt")
twitter <- read_lines("~/R/Capestone/data/final/en_US/en_US.twitter.txt")
blog <- tibble(text = blog)
news <- tibble(text = news)
twitter <- tibble(text = twitter)
set.seed(90210)
corpus <- full_join(blog,twitter) %>% full_join(news) %>%
slice_sample(prop = 0.10)
prof <- read_lines("~/R/Capestone/data/facebook-bad-words-list_comma-separated-text-file_2021_01_18.txt")[15]
prof <- prof %>% str_split(", ") %>% flatten %>% unlist
prof <- tibble("word" = prof)
english <- read_lines("~/R/Capestone/data/diction.txt")
english <- tibble("word" = english[!english==""])
#clean up ram
rm(blog,news,twitter)
voc <- english %>% anti_join(prof) %>% anti_join(stop_words)
unigram <- corpus %>% unnest_tokens(ngram, text, token = "ngrams", n = 1) %>%
semi_join(voc, by = c("ngram"="word"))
#decreases the voc size
voc <- tibble(word = unigram$ngram)
#OOV 1% of the least likely unigrams
unk <- unigram %>%
count(ngram) %>%
filter(n==1) %>%
slice_sample(prop = 0.005)
unigram[unigram$ngram %in% unk$ngram,]$ngram <- "<unk>"
unigram <- count(unigram, ngram)
remove <- unigram$n==1
remove
unigram
remove
sum(remove)
unigram <- unigram[!remove,]
unigram
voc <- tibble(word = unigram$ngram)
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = TRUE)
library(tidyverse)
library(tidytext)
library(stopwords)
#downloads the corpus files, profanity filter and English dictionary
url <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
url2 <- "https://www.freewebheaders.com/download/files/facebook-bad-words-list_comma-separated-text-file_2021_01_18.zip"
url3 <- "https://raw.githubusercontent.com/dwyl/english-words/master/words_alpha.txt"
if(dir.exists("~/R/Capestone/data/") == FALSE){
dir.create("~/R/Capestone/data/")}
if(file.exists("~/R/Capestone/data/data.zip") == FALSE|
file.exists("~/R/Capestone/data/prof.zip")==FALSE|
file.exists("~/R/Capestone/data/diction.txt")==FALSE){
download.file(url,destfile = "~/R/Capestone/data/data.zip")
download.file(url2,destfile = "~/R/Capestone/data/prof.zip")
download.file(url3,destfile = "~/R/Capestone/data/diction.txt")
setwd("~/R/Capestone/data/")
unzip("~/R/Capestone/data/prof.zip")
unzip("~/R/Capestone/data/data.zip")
setwd("~/R/Capestone")
}
blog <- read_lines("~/R/Capestone/data/final/en_US/en_US.blogs.txt")
news <- read_lines("~/R/Capestone/data/final/en_US/en_US.news.txt")
twitter <- read_lines("~/R/Capestone/data/final/en_US/en_US.twitter.txt")
blog <- tibble(text = blog)
news <- tibble(text = news)
twitter <- tibble(text = twitter)
set.seed(90210)
corpus <- full_join(blog,twitter) %>% full_join(news) %>%
slice_sample(prop = 0.10)
prof <- read_lines("~/R/Capestone/data/facebook-bad-words-list_comma-separated-text-file_2021_01_18.txt")[15]
prof <- prof %>% str_split(", ") %>% flatten %>% unlist
prof <- tibble("word" = prof)
english <- read_lines("~/R/Capestone/data/diction.txt")
english <- tibble("word" = english[!english==""])
#clean up ram
rm(blog,news,twitter)
voc <- english %>% anti_join(prof) %>% anti_join(stop_words)
unigram <- corpus %>% unnest_tokens(ngram, text, token = "ngrams", n = 1) %>%
semi_join(voc, by = c("ngram"="word"))
#decreases the voc size
voc <- tibble(word = unigram$ngram)
#OOV 1% of the least likely unigrams
unk <- unigram %>%
count(ngram) %>%
filter(n==1) %>%
slice_sample(prop = 0.005)
unigram[unigram$ngram %in% unk$ngram,]$ngram <- "<unk>"
unigram <- count(unigram, ngram)
remove <- unigram$n==1
unigram <- unigram[!remove,]
voc <- tibble(word = unigram$ngram)
bigram <- corpus %>% unnest_tokens(ngram, text, token = "ngrams", n = 2) %>%
separate(ngram, c("word1", "word2"))
