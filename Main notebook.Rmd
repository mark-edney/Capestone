---
title: "Natural_Language_Processing_Project"
output:
  pdf_document: default
  html_notebook: default
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = TRUE)
```

## Initialization

The inital step that loads the required libraries and downloads the data sets ifff
not all read on file. 
```{r initial}
library(tidyverse)
library(tidytext)


#downloads the corpus files, profanity filter and English dictionary

url <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
url2 <- "https://www.freewebheaders.com/download/files/facebook-bad-words-list_comma-separated-text-file_2021_01_18.zip"
url3 <- "https://raw.githubusercontent.com/dwyl/english-words/master/words_alpha.txt"
if(dir.exists("~/R/Capestone/data/") == FALSE){
       dir.create("~/R/Capestone/data/")}

if(file.exists("~/R/Capestone/data/data.zip") == FALSE|
   file.exists("~/R/Capestone/data/prof.zip")==FALSE|
   file.exists("~/R/Capestone/data/diction.txt")==FALSE){
        download.file(url,destfile = "~/R/Capestone/data/data.zip")
        download.file(url2,destfile = "~/R/Capestone/data/prof.zip")
        download.file(url3,destfile = "~/R/Capestone/data/diction.txt")
        setwd("~/R/Capestone/data/")
        unzip("~/R/Capestone/data/prof.zip")
        unzip("~/R/Capestone/data/data.zip")
        setwd("~/R/Capestone")
}
```

## Create corpus

At this stage the files are open and joined to create a corpus for the project. 
The Corpus is so large and requires some much ram that a sample of 20% is taken. 
```{r corpus}

blog <- read_lines("~/R/Capestone/data/final/en_US/en_US.blogs.txt")
news <- read_lines("~/R/Capestone/data/final/en_US/en_US.news.txt")
twitter <- read_lines("~/R/Capestone/data/final/en_US/en_US.twitter.txt")
blog <- tibble(text = blog) 
news <- tibble(text = news)
twitter <- tibble(text = twitter)

set.seed(90210)
corpus <- full_join(blog,twitter) %>% full_join(news) %>% 
        slice_sample(prop = 0.10) 
```

## Corpus filtering

Here the corpus filter is created to remove profanity and any word that is  not 
in the English dictionary.
```{r filter}
prof <- read_lines("~/R/Capestone/data/facebook-bad-words-list_comma-separated-text-file_2021_01_18.txt")[15]
prof <- prof %>% str_split(", ") %>% flatten %>% unlist
prof <- tibble("word" = prof)

english <- read_lines("~/R/Capestone/data/diction.txt")
english <- tibble("word" = english[!english==""])
```

## Vocabulary 

A vocabulary of words is created from the unique words with the applied filters
```{r vocab}
#clean up ram
rm(blog,news,twitter)
voc <- english %>% anti_join(prof)

unigram <- corpus %>% unnest_tokens(ngram, text, token = "ngrams", n = 1) %>%
        semi_join(voc, by = c("ngram"="word")) %>% count(ngram)
#decreases the voc size
voc <- tibble(word = unigram$ngram)
```


## Out of Vocabulary 

To model out of vocabulary words we take a sampling of the least frequent unigrams
and change them to the character "<unk>". If a word is tested that isn't in the vocabulary
for the corpus, the quantity will be converted to "<unk>".
```{r OOV}
#OOV 1% of the least likely unigrams
unks <- unigram[unigram$n==1,] %>% slice_sample(prop = 0.01)
unigram[unigram$ngram %in% unks$ngram,]$ngram <- "<unk>"
unigram <- count(unigram, ngram)
```

## Bigrams

The bigrams are created and then split into individual words which can be filtered
by the vocabulary list. 
```{r bigram}
bigram <- corpus %>% unnest_tokens(ngram, text, token = "ngrams", n = 2) %>%
        separate(ngram, c("word1", "word2")) %>%
        filter(word1 %in% voc$word, word2 %in% voc$word) 

bigram$word1[bigram$word1 %in% unks$ngram] <- "<unk>"
bigram$word2[bigram$word2 %in% unks$ngram] <- "<unk>"
bigram <- count(bigram, word1, word2)
```

## Trigram

Likewise, the trigrams are created in a similar manner
```{r trigrams}
trigram <- corpus %>% unnest_tokens(ngram, text, token = "ngrams", n = 3) %>%
        separate(ngram, c("word1", "word2", "word3")) %>% 
        filter(word1 %in% voc$word, word2 %in% voc$word, word3 %in% voc$word) 
trigram$word1[trigram$word1 %in% unks$ngram] <- "<unk>"
trigram$word2[trigram$word2 %in% unks$ngram] <- "<unk>"
trigram$word3[trigram$word3 %in% unks$ngram] <- "<unk>"
trigram <- count(trigram, word1, word2, word3)
```

## Testing

A tibble is made for model testing, the test set itself is from one of the quizes
```{r testdata}
dist <- tibble(word = voc$word)
input <- tibble(text = c("When you breathe, I want to be the air for you. I'll be there for you, I'd live and I'd",
                         "Guy at my table's wife got up to go to the bathroom and I asked about dessert and he started telling me about his",
                         "I'd give anything to see arctic monkeys this",
                         "Talking to your mom has the same effect as a hug and helps reduce your",
                         "When you were in Holland you were like 1 inch away from me but you hadn't time to take a",
                         "I'd just like all of these questions answered, a presentation of evidence, and a jury to settle the",
                         "I can't deal with unsymetrical things. I can't even hold an uneven number of bags of groceries in each",
                         "Every inch of you is perfect from the bottom to the",
                         "Iâ€™m thankful my childhood was filled with imagination and bruises from playing",
                         "I like how the same people are in almost all of Adam Sandler's")) %>%
        mutate(line = row_number())
input <- unnest_tokens(input, word, text)
```